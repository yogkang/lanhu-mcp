#!/usr/bin/env python3
"""
è“æ¹–Axureæ–‡æ¡£æå–MCPæœåŠ¡å™¨
ä½¿ç”¨FastMCPå®ç°
"""
import asyncio
import os
import re
import base64
import json
import hashlib
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Annotated, Optional, Union, List

# ä¸œå…«åŒºæ—¶åŒºï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
CHINA_TZ = timezone(timedelta(hours=8))
from urllib.parse import urlparse

# å…ƒæ•°æ®ç¼“å­˜é…ç½®ï¼ˆåŸºäºç‰ˆæœ¬å·çš„æ°¸ä¹…ç¼“å­˜ï¼‰
_metadata_cache = {}  # {cache_key: {'data': {...}, 'version_id': str}}

import httpx
from fastmcp import Context
from bs4 import BeautifulSoup
from fastmcp import FastMCP
from fastmcp.utilities.types import Image
from playwright.async_api import async_playwright

# åˆ›å»ºFastMCPæœåŠ¡å™¨
mcp = FastMCP("Lanhu Axure Extractor")

# å…¨å±€é…ç½®
DEFAULT_COOKIE = "your_lanhu_cookie_here"  # è¯·æ›¿æ¢ä¸ºä½ çš„è“æ¹–Cookieï¼Œä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·ä¸­è·å–

# ä»ç¯å¢ƒå˜é‡è¯»å–Cookieï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
COOKIE = os.getenv("LANHU_COOKIE", DEFAULT_COOKIE)

BASE_URL = "https://lanhuapp.com"
CDN_URL = "https://axure-file.lanhuapp.com"

# é£ä¹¦æœºå™¨äººWebhooké…ç½®ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡ï¼‰
DEFAULT_FEISHU_WEBHOOK = "https://open.feishu.cn/open-apis/bot/v2/hook/your-webhook-key-here"
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL", DEFAULT_FEISHU_WEBHOOK)

# æ•°æ®å­˜å‚¨ç›®å½•
DATA_DIR = Path(os.getenv("DATA_DIR", "./data"))
DATA_DIR.mkdir(parents=True, exist_ok=True)

# è§’è‰²æšä¸¾ï¼ˆç”¨äºè¯†åˆ«ç”¨æˆ·èº«ä»½ï¼‰
VALID_ROLES = ["åç«¯", "å‰ç«¯", "å®¢æˆ·ç«¯", "å¼€å‘", "è¿ç»´", "äº§å“", "é¡¹ç›®ç»ç†"]

# âš ï¸ @æé†’åªå…è®¸å…·ä½“äººåï¼Œç¦æ­¢ä½¿ç”¨è§’è‰²
# ç¤ºä¾‹äººååˆ—è¡¨ï¼Œè¯·æ ¹æ®ä½ çš„å›¢é˜Ÿæˆå‘˜ä¿®æ”¹
MENTION_ROLES = [
    "å¼ ä¸‰", "æå››", "ç‹äº”", "èµµå…­", "é’±ä¸ƒ", "å­™å…«",
    "å‘¨ä¹", "å´å", "éƒ‘åä¸€", "å†¯åäºŒ", "é™ˆåä¸‰", "è¤šåå››",
    "å«åäº”", "è’‹åå…­", "æ²ˆåä¸ƒ", "éŸ©åå…«", "æ¨åä¹", "æœ±äºŒå"
]

# é£ä¹¦ç”¨æˆ·IDæ˜ å°„
# ç¤ºä¾‹æ˜ å°„ï¼Œè¯·æ›¿æ¢ä¸ºä½ å›¢é˜Ÿæˆå‘˜çš„å®é™…é£ä¹¦ç”¨æˆ·ID
# é£ä¹¦ç”¨æˆ·IDå¯ä»¥é€šè¿‡é£ä¹¦å¼€æ”¾å¹³å°è·å–
FEISHU_USER_ID_MAP = {
    'å¼ ä¸‰': '0000000000000000001',
    'æå››': '0000000000000000002',
    'ç‹äº”': '0000000000000000003',
    'èµµå…­': '0000000000000000004',
    'é’±ä¸ƒ': '0000000000000000005',
    'å­™å…«': '0000000000000000006',
    'å‘¨ä¹': '0000000000000000007',
    'å´å': '0000000000000000008',
    'éƒ‘åä¸€': '0000000000000000009',
    'å†¯åäºŒ': '0000000000000000010',
    'é™ˆåä¸‰': '0000000000000000011',
    'è¤šåå››': '0000000000000000012',
    'å«åäº”': '0000000000000000013',
    'è’‹åå…­': '0000000000000000014',
    'æ²ˆåä¸ƒ': '0000000000000000015',
    'éŸ©åå…«': '0000000000000000016',
    'æ¨åä¹': '0000000000000000017',
    'æœ±äºŒå': '0000000000000000018',
}

# è§’è‰²æ˜ å°„è§„åˆ™ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼Œè¶Šå…·ä½“çš„è¶Šé å‰ï¼‰
ROLE_MAPPING_RULES = [
    # åç«¯ç›¸å…³
    (["åç«¯", "backend", "æœåŠ¡ç«¯", "server", "java", "php", "python", "go", "golang", "node", "nodejs", ".net", "c#"], "åç«¯"),
    # å‰ç«¯ç›¸å…³
    (["å‰ç«¯", "frontend", "h5", "web", "vue", "react", "angular", "javascript", "js", "ts", "typescript", "css"], "å‰ç«¯"),
    # å®¢æˆ·ç«¯ç›¸å…³ï¼ˆä¼˜å…ˆäº"å¼€å‘"ï¼‰
    (["å®¢æˆ·ç«¯", "client", "ios", "android", "å®‰å“", "ç§»åŠ¨ç«¯", "mobile", "app", "flutter", "rn", "react native", "swift", "kotlin", "objective-c", "oc"], "å®¢æˆ·ç«¯"),
    # è¿ç»´ç›¸å…³
    (["è¿ç»´", "ops", "devops", "sre", "dba", "è¿è¥ç»´æŠ¤", "ç³»ç»Ÿç®¡ç†", "infra", "infrastructure"], "è¿ç»´"),
    # äº§å“ç›¸å…³
    (["äº§å“", "product", "pm", "äº§å“ç»ç†", "éœ€æ±‚"], "äº§å“"),
    # é¡¹ç›®ç»ç†ç›¸å…³
    (["é¡¹ç›®ç»ç†", "é¡¹ç›®", "pmo", "project manager", "scrum", "æ•æ·"], "é¡¹ç›®ç»ç†"),
    # å¼€å‘ï¼ˆé€šç”¨ï¼Œä¼˜å…ˆçº§æœ€ä½ï¼‰
    (["å¼€å‘", "dev", "developer", "ç¨‹åºå‘˜", "coder", "engineer", "å·¥ç¨‹å¸ˆ"], "å¼€å‘"),
]


def normalize_role(role: str) -> str:
    """
    å°†ç”¨æˆ·è§’è‰²å½’ä¸€åŒ–åˆ°æ ‡å‡†è§’è‰²ç»„
    
    Args:
        role: ç”¨æˆ·åŸå§‹è§’è‰²åï¼ˆå¦‚ "phpåç«¯"ã€"iOSå¼€å‘"ï¼‰
    
    Returns:
        æ ‡å‡†è§’è‰²åï¼ˆå¦‚ "åç«¯"ã€"å®¢æˆ·ç«¯"ï¼‰
    """
    if not role:
        return "æœªçŸ¥"
    
    role_lower = role.lower()
    
    # å¦‚æœå·²ç»æ˜¯æ ‡å‡†è§’è‰²ï¼Œç›´æ¥è¿”å›
    if role in VALID_ROLES:
        return role
    
    # æŒ‰è§„åˆ™åŒ¹é…
    for keywords, standard_role in ROLE_MAPPING_RULES:
        for keyword in keywords:
            if keyword.lower() in role_lower:
                return standard_role
    
    # æ— æ³•åŒ¹é…ï¼Œè¿”å›åŸå€¼
    return role


def _get_metadata_cache_key(project_id: str, doc_id: str = None) -> str:
    """ç”Ÿæˆå…ƒæ•°æ®ç¼“å­˜é”®ï¼ˆä¸å«ç‰ˆæœ¬å·ï¼Œç”¨äºæŸ¥æ‰¾ï¼‰"""
    if doc_id:
        return f"{project_id}_{doc_id}"
    return project_id


def _get_cached_metadata(cache_key: str, version_id: str = None) -> Optional[dict]:
    """
    è·å–ç¼“å­˜çš„å…ƒæ•°æ®
    
    Args:
        cache_key: ç¼“å­˜é”®
        version_id: æ–‡æ¡£ç‰ˆæœ¬IDï¼Œå¦‚æœæä¾›åˆ™æ£€æŸ¥ç‰ˆæœ¬æ˜¯å¦åŒ¹é…
    
    Returns:
        ç¼“å­˜çš„å…ƒæ•°æ®ï¼Œå¦‚æœæœªå‘½ä¸­æˆ–ç‰ˆæœ¬ä¸åŒ¹é…åˆ™è¿”å›None
    """
    if cache_key in _metadata_cache:
        cache_entry = _metadata_cache[cache_key]
        
        # å¦‚æœæä¾›äº†version_idï¼Œæ£€æŸ¥ç‰ˆæœ¬æ˜¯å¦åŒ¹é…
        if version_id:
            if cache_entry.get('version_id') == version_id:
                return cache_entry['data']
            else:
                # ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œåˆ é™¤æ—§ç¼“å­˜
                del _metadata_cache[cache_key]
                return None
        
        # æ²¡æœ‰version_idï¼Œç›´æ¥è¿”å›ç¼“å­˜ï¼ˆç”¨äºé¡¹ç›®çº§åˆ«ç¼“å­˜ï¼‰
        return cache_entry['data']
    
    return None


def _set_cached_metadata(cache_key: str, metadata: dict, version_id: str = None):
    """
    è®¾ç½®ç¼“å­˜ï¼ˆåŸºäºç‰ˆæœ¬å·çš„æ°¸ä¹…ç¼“å­˜ï¼‰
    
    Args:
        cache_key: ç¼“å­˜é”®
        metadata: å…ƒæ•°æ®
        version_id: æ–‡æ¡£ç‰ˆæœ¬IDï¼Œå­˜å‚¨ååªè¦ç‰ˆæœ¬ä¸å˜å°±æ°¸ä¹…æœ‰æ•ˆ
    """
    _metadata_cache[cache_key] = {
        'data': metadata.copy(),
        'version_id': version_id  # ç‰ˆæœ¬å·ä½œä¸ºç¼“å­˜æœ‰æ•ˆæ€§æ ‡è¯†
    }


# ============================================
# é£ä¹¦æœºå™¨äººé€šçŸ¥åŠŸèƒ½
# ============================================

async def send_feishu_notification(
    summary: str,
    content: str,
    author_name: str,
    author_role: str,
    mentions: List[str],
    message_type: str,
    project_name: str = None,
    doc_name: str = None,
    doc_url: str = None
) -> bool:
    """
    å‘é€é£ä¹¦æœºå™¨äººé€šçŸ¥
    
    Args:
        summary: ç•™è¨€æ ‡é¢˜
        content: ç•™è¨€å†…å®¹
        author_name: ä½œè€…åç§°
        author_role: ä½œè€…è§’è‰²
        mentions: @çš„äººååˆ—è¡¨ï¼ˆå¿…é¡»æ˜¯å…·ä½“çš„äººåï¼Œä¸èƒ½æ˜¯è§’è‰²ï¼‰
        message_type: æ¶ˆæ¯ç±»å‹
        project_name: é¡¹ç›®åç§°
        doc_name: æ–‡æ¡£åç§°
        doc_url: æ–‡æ¡£é“¾æ¥
    
    Returns:
        bool: å‘é€æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    if not mentions:
        return False  # æ²¡æœ‰@ä»»ä½•äººï¼Œä¸å‘é€é€šçŸ¥
    
    # æ¶ˆæ¯ç±»å‹emojiæ˜ å°„
    type_emoji = {
        "normal": "ğŸ“¢",
        "task": "ğŸ“‹",
        "question": "â“",
        "urgent": "ğŸš¨",
        "knowledge": "ğŸ’¡"
    }
    
    emoji = type_emoji.get(message_type, "ğŸ“")
    
    # æ„å»ºé£ä¹¦@ç”¨æˆ·ä¿¡æ¯
    at_user_ids = []
    mention_names = []
    for name in mentions:
        user_id = FEISHU_USER_ID_MAP.get(name)
        if user_id:
            at_user_ids.append(user_id)
            mention_names.append(name)
    
    # é€’å½’æå–çº¯æ–‡æœ¬å†…å®¹
    def extract_text(obj):
        """é€’å½’æå–JSONä¸­çš„çº¯æ–‡æœ¬"""
        if isinstance(obj, str):
            # å°è¯•è§£æå­—ç¬¦ä¸²æ˜¯å¦ä¸ºJSON
            try:
                parsed = json.loads(obj)
                return extract_text(parsed)
            except:
                return obj
        elif isinstance(obj, list):
            texts = []
            for item in obj:
                text = extract_text(item)
                if text:
                    texts.append(text)
            return " ".join(texts)
        elif isinstance(obj, dict):
            # æå–textå­—æ®µ
            if "text" in obj:
                return extract_text(obj["text"])
            return ""
        else:
            return str(obj) if obj else ""
    
    plain_content = extract_text(content)
    
    # é™åˆ¶å†…å®¹é•¿åº¦
    if len(plain_content) > 500:
        plain_content = plain_content[:500] + "..."
    
    # æ„å»ºå¯Œæ–‡æœ¬å†…å®¹ï¼ˆä½¿ç”¨é£ä¹¦postæ ¼å¼æ”¯æŒ@åŠŸèƒ½ï¼‰
    content_list = [
        # å‘å¸ƒè€…ä¿¡æ¯
        [{"tag": "text", "text": f"ğŸ‘¤ å‘å¸ƒè€…ï¼š{author_name}ï¼ˆ{author_role}ï¼‰\n"}],
        # ç±»å‹
        [{"tag": "text", "text": f"ğŸ·ï¸ ç±»å‹ï¼š{message_type}\n"}],
    ]
    
    # @æé†’è¡Œï¼ˆå¦‚æœæœ‰@çš„äººï¼‰
    if at_user_ids:
        mention_line = [{"tag": "text", "text": "ğŸ“¨ æé†’ï¼š"}]
        for user_id, name in zip(at_user_ids, mention_names):
            mention_line.append({"tag": "at", "user_id": user_id})
            mention_line.append({"tag": "text", "text": " "})
        mention_line.append({"tag": "text", "text": "\n"})
        content_list.append(mention_line)
    
    # é¡¹ç›®ä¿¡æ¯
    if project_name:
        content_list.append([{"tag": "text", "text": f"ğŸ“ é¡¹ç›®ï¼š{project_name}\n"}])
    if doc_name:
        content_list.append([{"tag": "text", "text": f"ğŸ“„ æ–‡æ¡£ï¼š{doc_name}\n"}])
    
    # å†…å®¹
    content_list.append([{"tag": "text", "text": f"\nğŸ“ å†…å®¹ï¼š\n{plain_content}\n"}])
    
    # é“¾æ¥
    if doc_url:
        content_list.append([
            {"tag": "text", "text": "\nğŸ”— "},
            {"tag": "a", "text": "æŸ¥çœ‹éœ€æ±‚æ–‡æ¡£", "href": doc_url}
        ])
    
    # é£ä¹¦æ¶ˆæ¯payloadï¼ˆä½¿ç”¨å¯Œæ–‡æœ¬postæ ¼å¼ï¼‰
    payload = {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": summary,  # ç›´æ¥ä½¿ç”¨summaryï¼Œä¸å†æ·»åŠ emojiï¼ˆç”¨æˆ·è‡ªå·±ä¼šåŠ ï¼‰
                    "content": content_list
                }
            }
        }
    }
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(
                FEISHU_WEBHOOK_URL,
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
            result = response.json()
            
            # é£ä¹¦æˆåŠŸå“åº”: {"code":0,"msg":"success"}
            if result.get("code") == 0:
                if mention_names:
                    print(f"âœ… é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ: {summary} @{','.join(mention_names)}")
                else:
                    print(f"âœ… é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ: {summary}")
                return True
            else:
                print(f"âš ï¸ é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥: {result}")
                return False
                
    except Exception as e:
        print(f"âŒ é£ä¹¦é€šçŸ¥å‘é€å¼‚å¸¸: {e}")
        return False


# ============================================
# æ¶ˆæ¯å­˜å‚¨ç±»
# ============================================

class MessageStore:
    """æ¶ˆæ¯å­˜å‚¨ç®¡ç†ç±» - æ”¯æŒå›¢é˜Ÿç•™è¨€æ¿åŠŸèƒ½"""
    
    def __init__(self, project_id: str = None):
        """
        åˆå§‹åŒ–æ¶ˆæ¯å­˜å‚¨
        
        Args:
            project_id: é¡¹ç›®IDï¼Œå¦‚æœä¸ºNoneåˆ™ç”¨äºå…¨å±€æ“ä½œæ¨¡å¼
        """
        self.project_id = project_id
        self.storage_dir = DATA_DIR / "messages"
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        if project_id:
            self.file_path = self.storage_dir / f"{project_id}.json"
            self._data = self._load()
        else:
            # å…¨å±€æ¨¡å¼ï¼Œä¸åŠ è½½å•ä¸ªæ–‡ä»¶
            self.file_path = None
            self._data = None
    
    def _load(self) -> dict:
        """åŠ è½½é¡¹ç›®æ•°æ®"""
        if self.file_path.exists():
            try:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass
        return {
            "project_id": self.project_id,
            "next_id": 1,
            "messages": [],
            "collaborators": []
        }
    
    def _save(self):
        """ä¿å­˜é¡¹ç›®æ•°æ®"""
        with open(self.file_path, 'w', encoding='utf-8') as f:
            json.dump(self._data, f, ensure_ascii=False, indent=2)
    
    def _get_now(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²ï¼ˆä¸œå…«åŒº/åŒ—äº¬æ—¶é—´ï¼‰"""
        return datetime.now(CHINA_TZ).strftime("%Y-%m-%d %H:%M:%S")
    
    def _check_mentions_me(self, mentions: List[str], user_role: str) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦@äº†å½“å‰ç”¨æˆ·ï¼ˆæ”¯æŒè§’è‰²å½’ä¸€åŒ–åŒ¹é…ï¼‰"""
        if not mentions:
            return False
        if "æ‰€æœ‰äºº" in mentions:
            return True
        
        # å°†ç”¨æˆ·è§’è‰²å½’ä¸€åŒ–ååŒ¹é…
        normalized_user_role = normalize_role(user_role)
        
        # ç›´æ¥åŒ¹é…åŸå§‹è§’è‰²
        if user_role in mentions:
            return True
        
        # åŒ¹é…å½’ä¸€åŒ–åçš„è§’è‰²
        if normalized_user_role in mentions:
            return True
        
        return False
    
    def record_collaborator(self, name: str, role: str):
        """è®°å½•/æ›´æ–°åä½œè€…"""
        if not name or not role:
            return
        
        now = self._get_now()
        collaborators = self._data.get("collaborators", [])
        
        # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨
        for collab in collaborators:
            if collab["name"] == name and collab["role"] == role:
                collab["last_seen"] = now
                self._save()
                return
        
        # æ–°å¢åä½œè€…
        collaborators.append({
            "name": name,
            "role": role,
            "first_seen": now,
            "last_seen": now
        })
        self._data["collaborators"] = collaborators
        self._save()
    
    def get_collaborators(self) -> List[dict]:
        """è·å–åä½œè€…åˆ—è¡¨"""
        return self._data.get("collaborators", [])
    
    def save_message(self, summary: str, content: str, author_name: str, 
                     author_role: str, mentions: List[str] = None,
                     message_type: str = 'normal',
                     project_name: str = None, folder_name: str = None,
                     doc_id: str = None, doc_name: str = None,
                     doc_type: str = None, doc_version: str = None,
                     doc_updated_at: str = None, doc_url: str = None) -> dict:
        """
        ä¿å­˜æ–°æ¶ˆæ¯ï¼ˆåŒ…å«æ ‡å‡†å…ƒæ•°æ®ï¼‰
        
        Args:
            summary: æ¶ˆæ¯æ¦‚è¦
            content: æ¶ˆæ¯å†…å®¹
            author_name: ä½œè€…åç§°
            author_role: ä½œè€…è§’è‰²
            mentions: @çš„è§’è‰²åˆ—è¡¨
            message_type: ç•™è¨€ç±»å‹ (normal/task/question/urgent)
            project_name: é¡¹ç›®åç§°
            folder_name: æ–‡ä»¶å¤¹åç§°
            doc_id: æ–‡æ¡£ID
            doc_name: æ–‡æ¡£åç§°
            doc_type: æ–‡æ¡£ç±»å‹
            doc_version: æ–‡æ¡£ç‰ˆæœ¬
            doc_updated_at: æ–‡æ¡£æ›´æ–°æ—¶é—´
            doc_url: æ–‡æ¡£URL
        """
        msg_id = self._data["next_id"]
        self._data["next_id"] += 1
        
        now = self._get_now()
        message = {
            "id": msg_id,
            "summary": summary,
            "content": content,
            "mentions": mentions or [],
            "message_type": message_type,  # æ–°å¢ï¼šç•™è¨€ç±»å‹
            "author_name": author_name,
            "author_role": author_role,
            "created_at": now,
            "updated_at": None,
            "updated_by_name": None,
            "updated_by_role": None,
            
            # æ ‡å‡†å…ƒæ•°æ®ï¼ˆ10ä¸ªå­—æ®µï¼‰
            "project_id": self.project_id,
            "project_name": project_name,
            "folder_name": folder_name,
            "doc_id": doc_id,
            "doc_name": doc_name,
            "doc_type": doc_type,
            "doc_version": doc_version,
            "doc_updated_at": doc_updated_at,
            "doc_url": doc_url
        }
        
        self._data["messages"].append(message)
        self._save()
        return message
    
    def get_messages(self, user_role: str = None) -> List[dict]:
        """è·å–æ‰€æœ‰æ¶ˆæ¯ï¼ˆä¸å«contentï¼Œç”¨äºåˆ—è¡¨å±•ç¤ºï¼‰"""
        messages = []
        for msg in self._data.get("messages", []):
            msg_copy = {k: v for k, v in msg.items() if k != "content"}
            if user_role:
                msg_copy["mentions_me"] = self._check_mentions_me(msg.get("mentions", []), user_role)
            messages.append(msg_copy)
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        messages.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        return messages
    
    def get_message_by_id(self, msg_id: int, user_role: str = None) -> Optional[dict]:
        """æ ¹æ®IDè·å–æ¶ˆæ¯ï¼ˆå«contentï¼‰"""
        for msg in self._data.get("messages", []):
            if msg["id"] == msg_id:
                msg_copy = msg.copy()
                if user_role:
                    msg_copy["mentions_me"] = self._check_mentions_me(msg.get("mentions", []), user_role)
                return msg_copy
        return None
    
    def update_message(self, msg_id: int, editor_name: str, editor_role: str,
                       summary: str = None, content: str = None, 
                       mentions: List[str] = None) -> Optional[dict]:
        """æ›´æ–°æ¶ˆæ¯"""
        for msg in self._data.get("messages", []):
            if msg["id"] == msg_id:
                if summary is not None:
                    msg["summary"] = summary
                if content is not None:
                    msg["content"] = content
                if mentions is not None:
                    msg["mentions"] = mentions
                msg["updated_at"] = self._get_now()
                msg["updated_by_name"] = editor_name
                msg["updated_by_role"] = editor_role
                self._save()
                return msg
        return None
    
    def delete_message(self, msg_id: int) -> bool:
        """åˆ é™¤æ¶ˆæ¯"""
        messages = self._data.get("messages", [])
        for i, msg in enumerate(messages):
            if msg["id"] == msg_id:
                messages.pop(i)
                self._save()
                return True
        return False
    
    def get_all_messages(self, user_role: str = None) -> List[dict]:
        """
        è·å–æ‰€æœ‰é¡¹ç›®çš„ç•™è¨€ï¼ˆå…¨å±€æŸ¥è¯¢ï¼‰
        
        Args:
            user_role: ç”¨æˆ·è§’è‰²ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦@äº†è¯¥ç”¨æˆ·
        
        Returns:
            åŒ…å«æ‰€æœ‰é¡¹ç›®æ¶ˆæ¯çš„åˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
        """
        all_messages = []
        
        # éå†æ‰€æœ‰JSONæ–‡ä»¶
        for json_file in self.storage_dir.glob("*.json"):
            project_id = json_file.stem
            try:
                project_store = MessageStore(project_id)
                messages = project_store.get_messages(user_role=user_role)
                
                # æ¶ˆæ¯ä¸­å·²åŒ…å«å…ƒæ•°æ®ï¼Œç›´æ¥æ·»åŠ 
                all_messages.extend(messages)
            except Exception:
                # æŸä¸ªé¡¹ç›®åŠ è½½å¤±è´¥ä¸å½±å“å…¶ä»–é¡¹ç›®
                continue
        
        # å…¨å±€æ’åºï¼ˆæŒ‰åˆ›å»ºæ—¶é—´å€’åºï¼‰
        all_messages.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        return all_messages
    
    def get_all_messages_grouped(self, user_role: str = None, user_name: str = None) -> List[dict]:
        """
        è·å–æ‰€æœ‰é¡¹ç›®çš„ç•™è¨€ï¼ˆåˆ†ç»„è¿”å›ï¼ŒèŠ‚çœtokenï¼‰
        
        æŒ‰é¡¹ç›®+æ–‡æ¡£åˆ†ç»„ï¼Œæ¯ç»„çš„å…ƒæ•°æ®åªå‡ºç°ä¸€æ¬¡ï¼Œé¿å…é‡å¤
        
        Args:
            user_role: ç”¨æˆ·è§’è‰²ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦@äº†è¯¥ç”¨æˆ·
            user_name: ç”¨æˆ·åï¼Œç”¨äºåˆ¤æ–­æ¶ˆæ¯æ˜¯å¦æ˜¯è‡ªå·±å‘çš„
        
        Returns:
            åˆ†ç»„åˆ—è¡¨ï¼Œæ¯ç»„åŒ…å«å…ƒæ•°æ®å’Œè¯¥ç»„çš„æ¶ˆæ¯
        """
        # å…ˆè·å–æ‰€æœ‰æ¶ˆæ¯
        all_messages = self.get_all_messages(user_role)
        
        # æŒ‰ (project_id, doc_id) åˆ†ç»„
        from collections import defaultdict
        groups_dict = defaultdict(list)
        
        for msg in all_messages:
            # ç”Ÿæˆåˆ†ç»„é”®
            project_id = msg.get('project_id', 'unknown')
            doc_id = msg.get('doc_id', 'no_doc')
            group_key = f"{project_id}_{doc_id}"
            
            groups_dict[group_key].append(msg)
        
        # æ„å»ºåˆ†ç»„ç»“æœ
        groups = []
        for group_key, messages in groups_dict.items():
            if not messages:
                continue
            
            # ä»ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸­æå–å…ƒæ•°æ®ï¼ˆç»„å†…å…±äº«ï¼‰
            first_msg = messages[0]
            
            # æ„å»ºç»„ä¿¡æ¯
            group = {
                # å…ƒæ•°æ®ï¼ˆåªå‡ºç°ä¸€æ¬¡ï¼‰
                "project_id": first_msg.get('project_id'),
                "project_name": first_msg.get('project_name'),
                "folder_name": first_msg.get('folder_name'),
                "doc_id": first_msg.get('doc_id'),
                "doc_name": first_msg.get('doc_name'),
                "doc_type": first_msg.get('doc_type'),
                "doc_version": first_msg.get('doc_version'),
                "doc_updated_at": first_msg.get('doc_updated_at'),
                "doc_url": first_msg.get('doc_url'),
                
                # ç»Ÿè®¡ä¿¡æ¯
                "message_count": len(messages),
                "mentions_me_count": sum(1 for m in messages if m.get("mentions_me")),
                
                # æ¶ˆæ¯åˆ—è¡¨ï¼ˆç§»é™¤å…ƒæ•°æ®å­—æ®µï¼‰
                "messages": []
            }
            
            # ç§»é™¤æ¶ˆæ¯ä¸­çš„å…ƒæ•°æ®å­—æ®µï¼Œåªä¿ç•™æ ¸å¿ƒä¿¡æ¯
            meta_fields = {
                'project_id', 'project_name', 'folder_name',
                'doc_id', 'doc_name', 'doc_type', 'doc_version',
                'doc_updated_at', 'doc_url'
            }
            
            for msg in messages:
                # åˆ›å»ºç²¾ç®€æ¶ˆæ¯ï¼ˆä¸å«å…ƒæ•°æ®ï¼‰
                slim_msg = {k: v for k, v in msg.items() if k not in meta_fields}
                # æ¸…ç†nullå­—æ®µå¹¶æ·»åŠ is_edited/is_mineæ ‡å¿—
                slim_msg = _clean_message_dict(slim_msg, user_name)
                group["messages"].append(slim_msg)
            
            groups.append(group)
        
        # æŒ‰ç»„å†…æœ€æ–°æ¶ˆæ¯æ—¶é—´æ’åº
        groups.sort(
            key=lambda g: max((m.get('created_at', '') for m in g['messages']), default=''),
            reverse=True
        )
        
        return groups



def get_user_info(ctx: Context) -> tuple:
    """
    ä»URL queryå‚æ•°è·å–ç”¨æˆ·ä¿¡æ¯
    
    MCPè¿æ¥URLæ ¼å¼ï¼šhttp://xxx:port/mcp?role=åç«¯&name=äº‘é¹¤
    """
    try:
        # ä½¿ç”¨ FastMCP æä¾›çš„ get_http_request è·å–å½“å‰è¯·æ±‚
        from fastmcp.server.dependencies import get_http_request
        req = get_http_request()
        
        # ä» query å‚æ•°è·å–
        name = req.query_params.get('name', 'åŒ¿å')
        role = req.query_params.get('role', 'æœªçŸ¥')
        return name, role
    except Exception:
        pass
    return 'åŒ¿å', 'æœªçŸ¥'


def _clean_message_dict(msg: dict, current_user_name: str = None) -> dict:
    """
    æ¸…ç†æ¶ˆæ¯å­—å…¸ï¼Œç§»é™¤nullå€¼çš„æ›´æ–°å­—æ®µï¼Œå¹¶æ·»åŠ å¿«æ·æ ‡å¿—
    
    ä¼˜åŒ–ï¼š
    1. å¦‚æœæ¶ˆæ¯æœªè¢«ç¼–è¾‘ï¼Œçœç•¥ updated_at/updated_by_name/updated_by_role
    2. æ·»åŠ  is_edited æ ‡å¿—
    3. æ·»åŠ  is_mine æ ‡å¿—ï¼ˆå¦‚æœæä¾›äº†current_user_nameï¼‰
    """
    cleaned = msg.copy()
    
    # å¦‚æœæ¶ˆæ¯æœªè¢«ç¼–è¾‘ï¼Œçœç•¥è¿™äº›å­—æ®µ
    if cleaned.get('updated_at') is None:
        cleaned.pop('updated_at', None)
        cleaned.pop('updated_by_name', None)
        cleaned.pop('updated_by_role', None)
        cleaned['is_edited'] = False
    else:
        cleaned['is_edited'] = True
    
    # æ·»åŠ is_mineæ ‡å¿—
    if current_user_name:
        cleaned['is_mine'] = (cleaned.get('author_name') == current_user_name)
    
    return cleaned


def get_project_id_from_url(url: str) -> str:
    """ä»URLä¸­æå–project_id"""
    if not url or url.lower() == 'all':
        return None
    extractor = LanhuExtractor()
    params = extractor.parse_url(url)
    return params.get('project_id', '')


async def _fetch_metadata_from_url(url: str) -> dict:
    """
    ä»è“æ¹–URLè·å–æ ‡å‡†å…ƒæ•°æ®ï¼ˆ10ä¸ªå­—æ®µï¼‰- æ”¯æŒåŸºäºç‰ˆæœ¬å·çš„æ°¸ä¹…ç¼“å­˜
    
    Args:
        url: è“æ¹–URL
    
    Returns:
        åŒ…å«10ä¸ªå…ƒæ•°æ®å­—æ®µçš„å­—å…¸ï¼Œè·å–å¤±è´¥çš„å­—æ®µä¸ºNone
    """
    metadata = {
        'project_id': None,
        'project_name': None,
        'folder_name': None,
        'doc_id': None,
        'doc_name': None,
        'doc_type': None,
        'doc_version': None,
        'doc_updated_at': None,
        'doc_url': None
    }
    
    extractor = LanhuExtractor()
    try:
        params = extractor.parse_url(url)
        project_id = params.get('project_id')
        doc_id = params.get('doc_id')
        team_id = params.get('team_id')
        
        metadata['project_id'] = project_id
        metadata['doc_id'] = doc_id
        
        if not project_id:
            return metadata
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = _get_metadata_cache_key(project_id, doc_id)
        
        # å¦‚æœæœ‰doc_idï¼Œè·å–æ–‡æ¡£ä¿¡æ¯å’Œç‰ˆæœ¬å·
        version_id = None
        if doc_id:
            doc_info = await extractor.get_document_info(project_id, doc_id)
            
            # è·å–ç‰ˆæœ¬ID
            versions = doc_info.get('versions', [])
            if versions:
                version_id = versions[0].get('id')
                metadata['doc_version'] = versions[0].get('version_info')
            
            # æ£€æŸ¥ç¼“å­˜ï¼ˆåŸºäºç‰ˆæœ¬å·ï¼‰
            cached = _get_cached_metadata(cache_key, version_id)
            if cached:
                return cached
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œç»§ç»­è·å–æ•°æ®
            metadata['doc_name'] = doc_info.get('name')
            metadata['doc_type'] = doc_info.get('type', 'axure')
            
            # æ ¼å¼åŒ–æ›´æ–°æ—¶é—´
            update_time = doc_info.get('update_time')
            if update_time:
                try:
                    dt = datetime.fromisoformat(update_time.replace('Z', '+00:00'))
                    dt_china = dt.astimezone(CHINA_TZ)
                    metadata['doc_updated_at'] = dt_china.strftime('%Y-%m-%d %H:%M:%S')
                except Exception:
                    metadata['doc_updated_at'] = update_time
            
            # æ„å»ºæ–‡æ¡£URL
            if team_id and project_id and doc_id:
                metadata['doc_url'] = (
                    f"https://lanhuapp.com/web/#/item/project/product"
                    f"?tid={team_id}&pid={project_id}&docId={doc_id}"
                )
        
        # è·å–é¡¹ç›®ä¿¡æ¯
        if project_id and team_id:
            try:
                response = await extractor.client.get(
                    f"{BASE_URL}/api/project/multi_info",
                    params={
                        'project_id': project_id,
                        'team_id': team_id,
                        'doc_info': 1
                    }
                )
                if response.status_code == 200:
                    data = response.json()
                    if data.get('code') == '00000':
                        project_info = data.get('result', {})
                        metadata['project_name'] = project_info.get('name')
                        metadata['folder_name'] = project_info.get('folder_name')
            except Exception:
                pass
        
        # å­˜å…¥ç¼“å­˜ï¼ˆåŸºäºç‰ˆæœ¬å·ï¼‰
        _set_cached_metadata(cache_key, metadata, version_id)
    
    except Exception:
        pass
    finally:
        await extractor.close()
    
    return metadata



class LanhuExtractor:
    """è“æ¹–æå–å™¨"""

    CACHE_META_FILE = ".lanhu_cache.json"  # ç¼“å­˜å…ƒæ•°æ®æ–‡ä»¶å

    def __init__(self):
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Referer": "https://lanhuapp.com/web/",
            "Accept": "application/json, text/plain, */*",
            "Cookie": COOKIE,
            "sec-ch-ua": '"Chromium";v="142", "Google Chrome";v="142", "Not_A Brand";v="99"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"macOS"',
            "request-from": "web",
            "real-path": "/item/project/product"
        }
        self.client = httpx.AsyncClient(timeout=30.0, headers=headers, follow_redirects=True)

    def parse_url(self, url: str) -> dict:
        """
        è§£æè“æ¹–URLï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
        1. å®Œæ•´URL: https://lanhuapp.com/web/#/item/project/product?tid=...&pid=...
        2. å®Œæ•´URL: https://lanhuapp.com/web/#/item/project/stage?tid=...&pid=...
        3. å‚æ•°éƒ¨åˆ†: ?tid=...&pid=...
        4. å‚æ•°éƒ¨åˆ†ï¼ˆæ— ?ï¼‰: tid=...&pid=...

        Args:
            url: è“æ¹–URLæˆ–å‚æ•°å­—ç¬¦ä¸²

        Returns:
            åŒ…å«project_id, team_id, doc_id, version_idçš„å­—å…¸
        """
        # å¦‚æœæ˜¯å®Œæ•´URLï¼Œæå–fragmentéƒ¨åˆ†
        if url.startswith('http'):
            parsed = urlparse(url)
            fragment = parsed.fragment

            if not fragment:
                raise ValueError("Invalid Lanhu URL: missing fragment part")

            # ä»fragmentä¸­æå–å‚æ•°éƒ¨åˆ†
            if '?' in fragment:
                url = fragment.split('?', 1)[1]
            else:
                url = fragment

        # å¤„ç†åªæœ‰å‚æ•°çš„æƒ…å†µ
        if url.startswith('?'):
            url = url[1:]

        # è§£æå‚æ•°
        params = {}
        for part in url.split('&'):
            if '=' in part:
                key, value = part.split('=', 1)
                params[key] = value

        # æå–å¿…éœ€å‚æ•°
        team_id = params.get('tid')
        project_id = params.get('pid')
        doc_id = params.get('docId') or params.get('image_id')
        version_id = params.get('versionId')

        # éªŒè¯å¿…éœ€å‚æ•°
        if not project_id:
            raise ValueError(f"URL parsing failed: missing required param pid (project_id)")

        if not team_id:
            raise ValueError(f"URL parsing failed: missing required param tid (team_id)")

        return {
            'team_id': team_id,
            'project_id': project_id,
            'doc_id': doc_id,
            'version_id': version_id
        }

    async def get_document_info(self, project_id: str, doc_id: str) -> dict:
        """è·å–æ–‡æ¡£ä¿¡æ¯"""
        api_url = f"{BASE_URL}/api/project/image"
        params = {'pid': project_id, 'image_id': doc_id}

        response = await self.client.get(api_url, params=params)
        response.raise_for_status()

        data = response.json()
        code = data.get('code')
        success = (code == 0 or code == '0' or code == '00000')

        if not success:
            raise Exception(f"API Error: {data.get('msg')} (code={code})")

        return data.get('data') or data.get('result', {})

    def _get_cache_meta_path(self, output_dir: Path) -> Path:
        """è·å–ç¼“å­˜å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        return output_dir / self.CACHE_META_FILE

    def _load_cache_meta(self, output_dir: Path) -> dict:
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        meta_path = self._get_cache_meta_path(output_dir)
        if meta_path.exists():
            try:
                with open(meta_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}

    def _save_cache_meta(self, output_dir: Path, meta_data: dict):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
        meta_path = self._get_cache_meta_path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(meta_data, f, ensure_ascii=False, indent=2)

    def _check_file_integrity(self, output_dir: Path, expected_files: dict) -> dict:
        """
        æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§

        Args:
            output_dir: è¾“å‡ºç›®å½•
            expected_files: æœŸæœ›çš„æ–‡ä»¶å­—å…¸ {ç›¸å¯¹è·¯å¾„: md5ç­¾å}

        Returns:
            {
                'missing': [ç¼ºå¤±çš„æ–‡ä»¶åˆ—è¡¨],
                'corrupted': [æŸåçš„æ–‡ä»¶åˆ—è¡¨],
                'valid': [æœ‰æ•ˆçš„æ–‡ä»¶åˆ—è¡¨]
            }
        """
        result = {
            'missing': [],
            'corrupted': [],
            'valid': []
        }

        for rel_path, expected_md5 in expected_files.items():
            file_path = output_dir / rel_path

            if not file_path.exists():
                result['missing'].append(rel_path)
            elif expected_md5:
                # å¦‚æœæœ‰MD5ç­¾åï¼ŒéªŒè¯æ–‡ä»¶
                # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                # å®Œæ•´çš„MD5éªŒè¯ä¼šæ¯”è¾ƒæ…¢
                result['valid'].append(rel_path)
            else:
                result['valid'].append(rel_path)

        return result

    def _should_update_cache(self, output_dir: Path, current_version_id: str, project_mapping: dict) -> tuple:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç¼“å­˜

        Returns:
            (éœ€è¦æ›´æ–°, ç¼ºå¤±çš„æ–‡ä»¶åˆ—è¡¨)
        """
        cache_meta = self._load_cache_meta(output_dir)

        # æ£€æŸ¥ç‰ˆæœ¬
        cached_version = cache_meta.get('version_id')
        if cached_version != current_version_id:
            return (True, 'version_changed', [])

        # æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
        pages = project_mapping.get('pages', {})
        expected_files = {}

        # æ”¶é›†æ‰€æœ‰åº”è¯¥å­˜åœ¨çš„æ–‡ä»¶
        for html_filename in pages.keys():
            expected_files[html_filename] = None

        # æ£€æŸ¥å…³é”®ç›®å½•
        for key_dir in ['data', 'resources', 'files', 'images']:
            expected_files[key_dir] = None

        integrity = self._check_file_integrity(output_dir, expected_files)

        if integrity['missing']:
            return (True, 'files_missing', integrity['missing'])

        return (False, 'up_to_date', [])

    async def get_pages_list(self, url: str) -> dict:
        """è·å–æ–‡æ¡£çš„æ‰€æœ‰é¡µé¢åˆ—è¡¨ï¼ˆä»…åŒ…å«sitemapä¸­çš„é¡µé¢ï¼Œä¸Webç•Œé¢ä¸€è‡´ï¼‰"""
        params = self.parse_url(url)
        doc_info = await self.get_document_info(params['project_id'], params['doc_id'])

        # è·å–é¡¹ç›®è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«åˆ›å»ºè€…ç­‰ä¿¡æ¯ï¼‰
        project_info = None
        try:
            response = await self.client.get(
                f"{BASE_URL}/api/project/multi_info",
                params={
                    'project_id': params['project_id'],
                    'team_id': params['team_id'],
                    'doc_info': 1
                }
            )
            response.raise_for_status()
            data = response.json()
            if data.get('code') == '00000':
                project_info = data.get('result', {})
        except Exception:
            pass  # å¦‚æœè·å–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸºæœ¬ä¿¡æ¯

        # è·å–é¡¹ç›®çº§mapping JSON
        versions = doc_info.get('versions', [])
        if not versions:
            raise Exception("Document version info not found")

        latest_version = versions[0]
        json_url = latest_version.get('json_url')
        if not json_url:
            raise Exception("Mapping JSON URL not found")

        response = await self.client.get(json_url)
        response.raise_for_status()
        project_mapping = response.json()

        # ä»sitemapè·å–é¡µé¢åˆ—è¡¨ï¼ˆåªè¿”å›åœ¨å¯¼èˆªä¸­æ˜¾ç¤ºçš„é¡µé¢ï¼‰
        sitemap = project_mapping.get('sitemap', {})
        root_nodes = sitemap.get('rootNodes', [])

        # é€’å½’æå–æ‰€æœ‰é¡µé¢ï¼ˆä¿ç•™å±‚çº§ç»“æ„ï¼‰
        def extract_pages(nodes, pages_list, parent_path="", level=0, parent_folder=None):
            """
            é€’å½’æå–é¡µé¢ï¼Œä¿ç•™å±‚çº§ä¿¡æ¯
            
            æ ¹æ®çœŸå®è“æ¹–sitemapç»“æ„ï¼š
            - çº¯æ–‡ä»¶å¤¹ï¼štype="Folder" ä¸” url=""
            - é¡µé¢èŠ‚ç‚¹ï¼šæœ‰urlå­—æ®µï¼ˆtype="Wireframe"ç­‰ï¼‰
            - é¡µé¢å¯ä»¥æœ‰childrenï¼ˆå­é¡µé¢ï¼‰
            
            Args:
                nodes: å½“å‰å±‚çº§çš„èŠ‚ç‚¹åˆ—è¡¨
                pages_list: è¾“å‡ºçš„é¡µé¢åˆ—è¡¨
                parent_path: çˆ¶çº§è·¯å¾„ï¼ˆç”¨/åˆ†éš”ï¼‰
                level: å½“å‰å±‚çº§æ·±åº¦ï¼ˆ0ä¸ºæ ¹ï¼‰
                parent_folder: æ‰€å±æ–‡ä»¶å¤¹åç§°ï¼ˆæœ€è¿‘çš„FolderèŠ‚ç‚¹ï¼‰
            """
            for node in nodes:
                page_name = node.get('pageName', '')
                url = node.get('url', '')
                node_type = node.get('type', 'Wireframe')
                node_id = node.get('id', '')
                
                # æ„å»ºå½“å‰è·¯å¾„
                current_path = f"{parent_path}/{page_name}" if parent_path else page_name
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºçº¯æ–‡ä»¶å¤¹ï¼ˆtype=Folder ä¸” æ— urlï¼‰
                is_pure_folder = (node_type == 'Folder' and not url)
                
                if page_name and url:
                    # è¿™æ˜¯ä¸€ä¸ªé¡µé¢ï¼ˆæœ‰urlçš„éƒ½æ˜¯é¡µé¢ï¼‰
                    pages_list.append({
                        'index': len(pages_list) + 1,
                        'name': page_name,
                        'filename': url,
                        'id': node_id,
                        'type': node_type,
                        'level': level,
                        'folder': parent_folder or 'æ ¹ç›®å½•',  # æ‰€å±æ–‡ä»¶å¤¹
                        'path': current_path,  # å®Œæ•´è·¯å¾„
                        'has_children': bool(node.get('children'))  # æ˜¯å¦æœ‰å­é¡µé¢
                    })
                
                # é€’å½’å¤„ç†å­èŠ‚ç‚¹
                children = node.get('children', [])
                if children:
                    # å¦‚æœå½“å‰æ˜¯çº¯æ–‡ä»¶å¤¹ï¼Œæ›´æ–°parent_folder
                    # å¦‚æœå½“å‰æ˜¯é¡µé¢ï¼Œä¿æŒåŸparent_folder
                    next_folder = page_name if is_pure_folder else parent_folder
                    
                    extract_pages(
                        children, 
                        pages_list, 
                        parent_path=current_path,
                        level=level + 1,
                        parent_folder=next_folder
                    )

        pages_list = []
        extract_pages(root_nodes, pages_list)

        # æ ¼å¼åŒ–æ—¶é—´ï¼ˆè½¬æ¢ä¸ºä¸œå…«åŒº/åŒ—äº¬æ—¶é—´ï¼‰
        def format_time(time_str):
            if not time_str:
                return None
            try:
                # å¤„ç†ISOæ ¼å¼æ—¶é—´ï¼Œè½¬æ¢ä¸ºä¸œå…«åŒº
                dt = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                dt_china = dt.astimezone(CHINA_TZ)
                return dt_china.strftime('%Y-%m-%d %H:%M:%S')
            except Exception:
                return time_str

        # ç»Ÿè®¡åˆ†ç»„ä¿¡æ¯
        from collections import defaultdict
        folder_stats = defaultdict(int)
        max_level = 0
        pages_with_children = 0
        
        for page in pages_list:
            folder = page.get('folder', 'æ ¹ç›®å½•')
            folder_stats[folder] += 1
            max_level = max(max_level, page.get('level', 0))
            if page.get('has_children'):
                pages_with_children += 1
        
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            'document_id': params['doc_id'],
            'document_name': doc_info.get('name', 'Unknown'),
            'document_type': doc_info.get('type', 'axure'),
            'total_pages': len(pages_list),
            'max_level': max_level,
            'pages_with_children': pages_with_children,  # æœ‰å­é¡µé¢çš„é¡µé¢æ•°
            'folder_statistics': dict(folder_stats),  # æ¯ä¸ªæ–‡ä»¶å¤¹ä¸‹æœ‰å¤šå°‘é¡µé¢ï¼ˆæŒ‰çº¯Folderç»Ÿè®¡ï¼‰
            'pages': pages_list
        }

        # æ·»åŠ æ—¶é—´ä¿¡æ¯
        if doc_info.get('create_time'):
            result['create_time'] = format_time(doc_info.get('create_time'))
        if doc_info.get('update_time'):
            result['update_time'] = format_time(doc_info.get('update_time'))

        # æ·»åŠ ç‰ˆæœ¬ä¿¡æ¯
        result['total_versions'] = len(versions)
        if latest_version.get('version_info'):
            result['latest_version'] = latest_version.get('version_info')

        # æ·»åŠ é¡¹ç›®ä¿¡æ¯ï¼ˆå¦‚æœæˆåŠŸè·å–ï¼‰
        if project_info:
            if project_info.get('creator_name'):
                result['creator_name'] = project_info.get('creator_name')
            if project_info.get('folder_name'):
                result['folder_name'] = project_info.get('folder_name')
            if project_info.get('save_path'):
                result['project_path'] = project_info.get('save_path')
            if project_info.get('member_cnt'):
                result['member_count'] = project_info.get('member_cnt')

        return result

    async def download_resources(self, url: str, output_dir: str, force_update: bool = False) -> dict:
        """
        ä¸‹è½½æ‰€æœ‰Axureèµ„æºï¼ˆæ”¯æŒæ™ºèƒ½ç¼“å­˜ï¼‰

        Args:
            url: è“æ¹–æ–‡æ¡£URL
            output_dir: è¾“å‡ºç›®å½•
            force_update: å¼ºåˆ¶æ›´æ–°ï¼Œå¿½ç•¥ç¼“å­˜

        Returns:
            {
                'status': 'downloaded' | 'cached' | 'updated',
                'version_id': ç‰ˆæœ¬ID,
                'reason': æ›´æ–°åŸå› ,
                'output_dir': è¾“å‡ºç›®å½•
            }
        """
        params = self.parse_url(url)
        doc_info = await self.get_document_info(params['project_id'], params['doc_id'])

        # è·å–é¡¹ç›®çº§mapping JSON
        versions = doc_info.get('versions', [])
        version_info = versions[0]
        version_id = version_info.get('id', '')  # ç‰ˆæœ¬IDå­—æ®µåæ˜¯'id'
        json_url = version_info.get('json_url')

        response = await self.client.get(json_url)
        response.raise_for_status()
        project_mapping = response.json()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if not force_update and output_path.exists():
            need_update, reason, missing_files = self._should_update_cache(
                output_path, version_id, project_mapping
            )

            if not need_update:
                return {
                    'status': 'cached',
                    'version_id': version_id,
                    'reason': reason,
                    'output_dir': output_dir
                }

            # å¦‚æœåªæ˜¯æ–‡ä»¶ç¼ºå¤±ï¼Œå¯ä»¥å¢é‡ä¸‹è½½
            if reason == 'files_missing' and missing_files:
                # è¿™é‡Œå¯ä»¥å®ç°å¢é‡ä¸‹è½½é€»è¾‘
                # ä¸ºäº†ç®€åŒ–ï¼Œæš‚æ—¶è¿˜æ˜¯å…¨é‡ä¸‹è½½
                pass

        output_path.mkdir(parents=True, exist_ok=True)

        # ä¸‹è½½æ¯ä¸ªé¡µé¢çš„èµ„æº
        pages = project_mapping.get('pages', {})
        is_first_page = True

        downloaded_files = []

        for html_filename, page_info in pages.items():
            html_data = page_info.get('html', {})
            html_file_with_md5 = html_data.get('sign_md5', '')
            page_mapping_md5 = page_info.get('mapping_md5', '')

            if not html_file_with_md5:
                continue

            # ä¸‹è½½HTML
            html_url = f"{CDN_URL}/{html_file_with_md5}"
            response = await self.client.get(html_url)
            response.raise_for_status()
            html_content = response.text

            # ä¸‹è½½é¡µé¢çº§mapping JSON
            if page_mapping_md5:
                mapping_url = f"{CDN_URL}/{page_mapping_md5}"
                response = await self.client.get(mapping_url)
                response.raise_for_status()
                page_mapping = response.json()

                # ä¸‹è½½æ‰€æœ‰ä¾èµ–èµ„æº
                await self._download_page_resources(
                    page_mapping, output_path, skip_document_js=(not is_first_page)
                )
                is_first_page = False

            # ä¿å­˜HTML
            html_path = output_path / html_filename
            html_path.write_text(html_content, encoding='utf-8')
            downloaded_files.append(html_filename)

        # ä¿å­˜ç¼“å­˜å…ƒæ•°æ®
        cache_meta = {
            'version_id': version_id,
            'document_id': params['doc_id'],
            'document_name': doc_info.get('name', 'Unknown'),
            'download_time': asyncio.get_event_loop().time(),
            'pages': list(pages.keys()),
            'total_files': len(downloaded_files)
        }
        self._save_cache_meta(output_path, cache_meta)

        return {
            'status': 'downloaded',
            'version_id': version_id,
            'reason': 'first_download' if not output_path.exists() else 'version_changed',
            'output_dir': output_dir
        }

    async def _download_page_resources(self, page_mapping: dict, output_dir: Path, skip_document_js: bool = False):
        """ä¸‹è½½é¡µé¢èµ„æº"""
        tasks = []

        # ä¸‹è½½CSS
        for local_path, info in page_mapping.get('styles', {}).items():
            sign_md5 = info.get('sign_md5', '')
            if sign_md5:
                url = sign_md5 if sign_md5.startswith('http') else f"{CDN_URL}/{sign_md5}"
                tasks.append(self._download_file(url, output_dir / local_path))

        # ä¸‹è½½JS
        for local_path, info in page_mapping.get('scripts', {}).items():
            if skip_document_js and local_path == 'data/document.js':
                continue
            sign_md5 = info.get('sign_md5', '')
            if sign_md5:
                url = sign_md5 if sign_md5.startswith('http') else f"{CDN_URL}/{sign_md5}"
                tasks.append(self._download_file(url, output_dir / local_path))

        # ä¸‹è½½å›¾ç‰‡
        for local_path, info in page_mapping.get('images', {}).items():
            sign_md5 = info.get('sign_md5', '')
            if sign_md5:
                url = sign_md5 if sign_md5.startswith('http') else f"{CDN_URL}/{sign_md5}"
                tasks.append(self._download_file(url, output_dir / local_path))

        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

    async def _download_file(self, url: str, local_path: Path):
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
        try:
            local_path.parent.mkdir(parents=True, exist_ok=True)
            response = await self.client.get(url)
            response.raise_for_status()
            local_path.write_bytes(response.content)
        except Exception:
            pass

    async def get_design_slices_info(self, image_id: str, team_id: str, project_id: str,
                                     include_metadata: bool = True) -> dict:
        """
        è·å–è®¾è®¡å›¾çš„æ‰€æœ‰åˆ‡å›¾ä¿¡æ¯ï¼ˆä»…è¿”å›å…ƒæ•°æ®å’Œä¸‹è½½åœ°å€ï¼Œä¸ä¸‹è½½æ–‡ä»¶ï¼‰

        Args:
            image_id: è®¾è®¡å›¾ID
            team_id: å›¢é˜ŸID
            project_id: é¡¹ç›®ID
            include_metadata: æ˜¯å¦åŒ…å«è¯¦ç»†å…ƒæ•°æ®ï¼ˆä½ç½®ã€é¢œè‰²ã€æ ·å¼ç­‰ï¼‰

        Returns:
            åŒ…å«åˆ‡å›¾åˆ—è¡¨å’Œè¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        # 1. è·å–è®¾è®¡å›¾è¯¦æƒ…
        url = f"{BASE_URL}/api/project/image"
        params = {
            "dds_status": 1,
            "image_id": image_id,
            "team_id": team_id,
            "project_id": project_id
        }
        response = await self.client.get(url, params=params)
        data = response.json()

        if data['code'] != '00000':
            raise Exception(f"Failed to get design: {data['msg']}")

        result = data['result']
        latest_version = result['versions'][0]
        json_url = latest_version['json_url']

        # 2. ä¸‹è½½å¹¶è§£æSketch JSON
        json_response = await self.client.get(json_url)
        sketch_data = json_response.json()

        # 3. é€’å½’æå–æ‰€æœ‰åˆ‡å›¾
        slices = []

        def find_dds_images(obj, parent_name="", layer_path=""):
            if not obj or not isinstance(obj, dict):
                return

            current_name = obj.get('name', '')
            current_path = f"{layer_path}/{current_name}" if layer_path else current_name

            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ‡å›¾
            if obj.get('ddsImage') and obj['ddsImage'].get('imageUrl'):
                slice_info = {
                    'id': obj.get('id'),
                    'name': current_name,
                    'type': obj.get('type') or obj.get('ddsType'),
                    'download_url': obj['ddsImage']['imageUrl'],
                    'size': obj['ddsImage']['size'],
                }

                # æ·»åŠ ä½ç½®ä¿¡æ¯
                if 'left' in obj and 'top' in obj:
                    slice_info['position'] = {
                        'x': int(obj.get('left', 0)),
                        'y': int(obj.get('top', 0))
                    }

                # æ·»åŠ çˆ¶å›¾å±‚ä¿¡æ¯
                if parent_name:
                    slice_info['parent_name'] = parent_name

                slice_info['layer_path'] = current_path

                # å¦‚æœéœ€è¦è¯¦ç»†å…ƒæ•°æ®
                if include_metadata:
                    metadata = {}

                    # å¡«å……é¢œè‰²
                    if obj.get('fills'):
                        metadata['fills'] = obj['fills']

                    # è¾¹æ¡†
                    if obj.get('borders'):
                        metadata['borders'] = obj['borders']

                    # é€æ˜åº¦
                    if 'opacity' in obj:
                        metadata['opacity'] = obj['opacity']

                    # æ—‹è½¬
                    if obj.get('rotation'):
                        metadata['rotation'] = obj['rotation']

                    # æ–‡æœ¬æ ·å¼
                    if obj.get('textStyle'):
                        metadata['text_style'] = obj['textStyle']

                    # é˜´å½±
                    if obj.get('shadows'):
                        metadata['shadows'] = obj['shadows']

                    # åœ†è§’
                    if obj.get('radius'):
                        metadata['border_radius'] = obj['radius']

                    if metadata:
                        slice_info['metadata'] = metadata

                slices.append(slice_info)

            # é€’å½’å¤„ç†å­å›¾å±‚
            if obj.get('layers'):
                for layer in obj['layers']:
                    find_dds_images(layer, current_name, current_path)

            # é€’å½’å¤„ç†æ‰€æœ‰å¯¹è±¡å±æ€§
            for value in obj.values():
                if isinstance(value, dict) and value != obj:
                    find_dds_images(value, parent_name, layer_path)
                elif isinstance(value, list):
                    for item in value:
                        if isinstance(item, dict):
                            find_dds_images(item, parent_name, layer_path)

        # ä»infoæ•°ç»„å¼€å§‹æŸ¥æ‰¾
        if sketch_data.get('info'):
            for item in sketch_data['info']:
                find_dds_images(item)

        return {
            'design_id': image_id,
            'design_name': result['name'],
            'version': latest_version['version_info'],
            'canvas_size': {
                'width': result.get('width'),
                'height': result.get('height')
            },
            'total_slices': len(slices),
            'slices': slices
        }

    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self.client.aclose()


def fix_html_files(directory: str):
    """ä¿®å¤HTMLæ–‡ä»¶"""
    html_files = list(Path(directory).glob("*.html"))

    for html_path in html_files:
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()

        soup = BeautifulSoup(content, 'html.parser')

        # æ›¿æ¢data-src
        for tag in soup.find_all(['img', 'script']):
            if tag.has_attr('data-src'):
                tag['src'] = tag['data-src']
                del tag['data-src']

        for tag in soup.find_all('link'):
            if tag.has_attr('data-src'):
                tag['href'] = tag['data-src']
                del tag['data-src']

        # ç§»é™¤bodyéšè—æ ·å¼
        body = soup.find('body')
        if body and body.has_attr('style'):
            style = body['style']
            style = re.sub(r'display\s*:\s*none\s*;?', '', style)
            style = re.sub(r'opacity\s*:\s*0\s*;?', '', style)
            style = style.strip()
            if style:
                body['style'] = style
            else:
                del body['style']

        # ç§»é™¤è“æ¹–è„šæœ¬
        for script in soup.find_all('script'):
            if script.string and 'alistatic.lanhuapp.com' in script.string:
                script.decompose()

        # æ·»åŠ æ˜ å°„å‡½æ•°
        head = soup.find('head')
        if head:
            mapping_script = soup.new_tag('script')
            mapping_script.string = '''
// è“æ¹–Axureæ˜ å°„æ•°æ®å¤„ç†å‡½æ•°
function lanhu_Axure_Mapping_Data(data) {
    return data;
}
'''
            first_script = head.find('script')
            if first_script:
                first_script.insert_before(mapping_script)
            else:
                head.append(mapping_script)

        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(str(soup))


async def screenshot_page_internal(resource_dir: str, page_names: List[str], output_dir: str,
                                   return_base64: bool = True, version_id: str = None) -> List[dict]:
    """å†…éƒ¨æˆªå›¾å‡½æ•°ï¼ˆåŒæ—¶æå–é¡µé¢æ–‡æœ¬ï¼‰ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜"""
    import http.server
    import socketserver
    import threading
    import time

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ç¼“å­˜å…ƒæ•°æ®æ–‡ä»¶
    cache_meta_path = output_path / ".screenshot_cache.json"
    cache_meta = {}
    if cache_meta_path.exists():
        try:
            with open(cache_meta_path, 'r', encoding='utf-8') as f:
                cache_meta = json.load(f)
        except Exception:
            cache_meta = {}
    
    # æ£€æŸ¥å“ªäº›é¡µé¢éœ€è¦é‡æ–°æˆªå›¾
    cached_version = cache_meta.get('version_id')
    pages_to_render = []
    cached_results = []
    
    for page_name in page_names:
        safe_name = re.sub(r'[^\w\s-]', '_', page_name)
        screenshot_file = output_path / f"{safe_name}.png"
        text_file = output_path / f"{safe_name}.txt"
        
        # å¦‚æœç‰ˆæœ¬ç›¸åŒä¸”æ–‡ä»¶å­˜åœ¨ï¼Œå¤ç”¨ç¼“å­˜
        if (version_id and cached_version == version_id and 
            screenshot_file.exists()):
            # è¯»å–ç¼“å­˜çš„æ–‡æœ¬å†…å®¹
            page_text = ""
            if text_file.exists():
                try:
                    page_text = text_file.read_text(encoding='utf-8')
                except Exception:
                    page_text = "(Cached - text not available)"
            
            cached_results.append({
                'page_name': page_name,
                'success': True,
                'screenshot_path': str(screenshot_file),
                'page_text': page_text if page_text else "(Cached result)",
                'size': f"{screenshot_file.stat().st_size / 1024:.1f}KB",
                'from_cache': True
            })
        else:
            pages_to_render.append(page_name)
    
    results = list(cached_results)
    
    # å¦‚æœæ‰€æœ‰é¡µé¢éƒ½æœ‰ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    if not pages_to_render:
        return results
    
    # å¯åŠ¨HTTPæœåŠ¡å™¨ï¼ˆåªæœ‰éœ€è¦æ¸²æŸ“æ—¶æ‰å¯åŠ¨ï¼‰
    import random
    port = random.randint(8800, 8900)
    abs_dir = os.path.abspath(resource_dir)
    handler = lambda *args, **kwargs: http.server.SimpleHTTPRequestHandler(
        *args, directory=abs_dir, **kwargs
    )
    httpd = socketserver.TCPServer(("", port), handler)
    thread = threading.Thread(target=httpd.serve_forever, daemon=True)
    thread.start()
    time.sleep(1)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page(viewport={'width': 1920, 'height': 1080})

        for page_name in pages_to_render:
            try:
                # æŸ¥æ‰¾HTMLæ–‡ä»¶
                html_file = None
                for f in Path(resource_dir).glob("*.html"):
                    if f.stem == page_name:
                        html_file = f.name
                        break

                if not html_file:
                    results.append({
                        'page_name': page_name,
                        'success': False,
                        'error': f'Page {page_name} does not exist'
                    })
                    continue

                # è®¿é—®é¡µé¢
                url = f"http://localhost:{port}/{html_file}"
                await page.goto(url, wait_until='networkidle', timeout=30000)
                await page.wait_for_timeout(2000)

                # Extract page text content (optimized for Axure)
                page_text = await page.evaluate('''() => {
                    let sections = [];

                    // 1. Extract red annotation/warning text (product key notes)
                    const redTexts = Array.from(document.querySelectorAll('*')).filter(el => {
                        const style = window.getComputedStyle(el);
                        const color = style.color;
                        // Detect red text (rgb(255,0,0) or #ff0000, etc.)
                        return color && (
                            color.includes('rgb(255, 0, 0)') || 
                            color.includes('rgb(255,0,0)') ||
                            color === 'red'
                        );
                    });

                    if (redTexts.length > 0) {
                        const redContent = redTexts
                            .map(el => el.textContent.trim())
                            .filter(t => t.length > 0 && t.length < 200)
                            .filter((v, i, a) => a.indexOf(v) === i); // dedupe
                        if (redContent.length > 0) {
                            sections.push("[Important Tips/Warnings]\\n" + redContent.join("\\n"));
                        }
                    }

                    // 2. Extract Axure shape/flowchart node text
                    const axureShapes = document.querySelectorAll('[id^="u"], .ax_shape, .shape, [class*="shape"]');
                    const shapeTexts = [];
                    axureShapes.forEach(el => {
                        const text = el.textContent.trim();
                        // Only text with appropriate length (avoid overly long paragraphs)
                        if (text && text.length > 0 && text.length < 100) {
                            shapeTexts.push(text);
                        }
                    });

                    if (shapeTexts.length > 5) { // If many shape texts extracted, likely a flowchart
                        const uniqueShapes = [...new Set(shapeTexts)];
                        sections.push("[Flowchart/Component Text]\\n" + uniqueShapes.slice(0, 20).join(" | ")); // max 20
                    }

                    // 3. Extract all visible text (most complete content)
                    const bodyText = document.body.innerText || '';
                    if (bodyText.trim()) {
                        sections.push("[Full Page Text]\\n" + bodyText.trim());
                    }

                    // 4. If nothing extracted
                    if (sections.length === 0) {
                        return "âš ï¸ Page text is empty or cannot be extracted (please refer to visual output)";
                    }

                    return sections.join("\\n\\n");
                }''')

                # æˆªå›¾
                safe_name = re.sub(r'[^\w\s-]', '_', page_name)
                screenshot_path = output_path / f"{safe_name}.png"
                text_path = output_path / f"{safe_name}.txt"

                # è·å–æˆªå›¾å­—èŠ‚
                screenshot_bytes = await page.screenshot(full_page=True)

                # ä¿å­˜æˆªå›¾åˆ°æ–‡ä»¶
                screenshot_path.write_bytes(screenshot_bytes)
                
                # ä¿å­˜æ–‡æœ¬åˆ°æ–‡ä»¶ï¼ˆç”¨äºç¼“å­˜ï¼‰
                try:
                    text_path.write_text(page_text, encoding='utf-8')
                except Exception:
                    pass

                result = {
                    'page_name': page_name,
                    'success': True,
                    'screenshot_path': str(screenshot_path),
                    'page_text': page_text,
                    'size': f"{len(screenshot_bytes) / 1024:.1f}KB",
                    'from_cache': False
                }

                # å¦‚æœéœ€è¦è¿”å›base64
                if return_base64:
                    result['base64'] = base64.b64encode(screenshot_bytes).decode('utf-8')
                    result['mime_type'] = 'image/png'

                results.append(result)
            except Exception as e:
                results.append({
                    'page_name': page_name,
                    'success': False,
                    'error': str(e)
                })

        await browser.close()

    # åœæ­¢æœåŠ¡å™¨
    httpd.shutdown()
    httpd.server_close()
    
    # æ›´æ–°ç¼“å­˜å…ƒæ•°æ®
    if version_id:
        cache_meta['version_id'] = version_id
        cache_meta['cached_pages'] = page_names
        try:
            with open(cache_meta_path, 'w', encoding='utf-8') as f:
                json.dump(cache_meta, f, ensure_ascii=False, indent=2)
        except Exception:
            pass

    return results


@mcp.tool()
async def lanhu_resolve_invite_link(
    invite_url: Annotated[str, "Lanhu invite link. Example: https://lanhuapp.com/link/#/invite?sid=xxx"]
) -> dict:
    """
    Resolve Lanhu invite/share link to actual project URL
    
    USE THIS WHEN: User provides invite link (lanhuapp.com/link/#/invite?sid=xxx)
    
    Purpose: Convert invite link to usable project URL with tid/pid/docId parameters
    
    Returns:
        Resolved URL and parsed parameters
    """
    try:
        # è§£æCookieå­—ç¬¦ä¸²ä¸ºplaywrightæ ¼å¼
        cookies = []
        for cookie_str in COOKIE.split('; '):
            if '=' in cookie_str:
                name, value = cookie_str.split('=', 1)
                cookies.append({
                    'name': name,
                    'value': value,
                    'domain': '.lanhuapp.com',
                    'path': '/'
                })
        
        # ä½¿ç”¨playwrightæ¥å¤„ç†å‰ç«¯é‡å®šå‘
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()
            
            # æ·»åŠ cookies
            if cookies:
                await context.add_cookies(cookies)
            
            page = await context.new_page()
            
            # è®¿é—®é‚€è¯·é“¾æ¥ï¼Œç­‰å¾…é‡å®šå‘å®Œæˆ
            await page.goto(invite_url, wait_until='networkidle', timeout=30000)
            
            # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿é‡å®šå‘å®Œæˆ
            await page.wait_for_timeout(2000)
            
            # è·å–æœ€ç»ˆURL
            final_url = page.url
            
            await browser.close()
            
            # è§£ææœ€ç»ˆURL
            extractor = LanhuExtractor()
            try:
                params = extractor.parse_url(final_url)
                
                return {
                    "status": "success",
                    "invite_url": invite_url,
                    "resolved_url": final_url,
                    "parsed_params": params,
                    "usage_tip": "You can now use this resolved_url with other lanhu tools (lanhu_get_pages, lanhu_get_designs, etc.)"
                }
            except Exception as e:
                return {
                    "status": "partial_success",
                    "invite_url": invite_url,
                    "resolved_url": final_url,
                    "parse_error": str(e),
                    "message": "URL resolved but parsing failed. You can try using the resolved_url directly."
                }
            finally:
                await extractor.close()
                
    except Exception as e:
        return {
            "status": "error",
            "invite_url": invite_url,
            "error": str(e),
            "message": "Failed to resolve invite link. Please check if the link is valid."
        }


def _get_analysis_mode_options_by_role(user_role: str) -> str:
    """
    æ ¹æ®ç”¨æˆ·è§’è‰²ç”Ÿæˆåˆ†ææ¨¡å¼é€‰é¡¹ï¼ˆè°ƒæ•´æ¨èé¡ºåºï¼‰
    
    Args:
        user_role: ç”¨æˆ·è§’è‰²
    
    Returns:
        æ ¼å¼åŒ–çš„é€‰é¡¹æ–‡æœ¬
    """
    # å½’ä¸€åŒ–è§’è‰²
    normalized_role = normalize_role(user_role)
    
    # å®šä¹‰ä¸‰ç§æ¨¡å¼çš„å®Œæ•´æè¿°
    developer_option = """1ï¸âƒ£ ã€å¼€å‘è§†è§’ã€‘- è¯¦ç»†æŠ€æœ¯æ–‡æ¡£
   é€‚åˆï¼šå¼€å‘äººå‘˜çœ‹éœ€æ±‚ï¼Œå‡†å¤‡å†™ä»£ç 
   è¾“å‡ºå†…å®¹ï¼š
   - è¯¦ç»†å­—æ®µè§„åˆ™è¡¨ï¼ˆå¿…å¡«ã€ç±»å‹ã€é•¿åº¦ã€æ ¡éªŒè§„åˆ™ã€æç¤ºæ–‡æ¡ˆï¼‰
   - ä¸šåŠ¡è§„åˆ™æ¸…å•ï¼ˆåˆ¤æ–­æ¡ä»¶ã€å¼‚å¸¸å¤„ç†ã€æ•°æ®æµå‘ï¼‰
   - å…¨å±€æµç¨‹å›¾ï¼ˆåŒ…å«æ‰€æœ‰åˆ†æ”¯ã€åˆ¤æ–­ã€å¼‚å¸¸å¤„ç†ï¼‰
   - æ¥å£ä¾èµ–è¯´æ˜ã€æ•°æ®åº“è®¾è®¡å»ºè®®"""
    
    tester_option = """2ï¸âƒ£ ã€æµ‹è¯•è§†è§’ã€‘- æµ‹è¯•ç”¨ä¾‹å’ŒéªŒè¯ç‚¹
   é€‚åˆï¼šæµ‹è¯•äººå‘˜å†™æµ‹è¯•ç”¨ä¾‹
   è¾“å‡ºå†…å®¹ï¼š
   - æ­£å‘æµ‹è¯•åœºæ™¯ï¼ˆå‰ç½®æ¡ä»¶â†’æ­¥éª¤â†’æœŸæœ›ç»“æœï¼‰
   - å¼‚å¸¸æµ‹è¯•åœºæ™¯ï¼ˆè¾¹ç•Œå€¼ã€å¼‚å¸¸æƒ…å†µã€é”™è¯¯æç¤ºï¼‰
   - å­—æ®µæ ¡éªŒè§„åˆ™è¡¨ï¼ˆå«æµ‹è¯•è¾¹ç•Œå€¼ï¼‰
   - çŠ¶æ€å˜åŒ–æµ‹è¯•ç‚¹ã€è”è°ƒæµ‹è¯•æ¸…å•"""
    
    explorer_option = """3ï¸âƒ£ ã€å¿«é€Ÿæ¢ç´¢ã€‘- å…¨å±€è¯„å®¡è§†è§’
   é€‚åˆï¼šéœ€æ±‚è¯„å®¡ä¼šè®®ã€å¿«é€Ÿäº†è§£éœ€æ±‚
   è¾“å‡ºå†…å®¹ï¼š
   - æ¨¡å—æ ¸å¿ƒåŠŸèƒ½æ¦‚è§ˆï¼ˆ3-5ä¸ªå…³é”®ç‚¹ï¼‰
   - æ¨¡å—ä¾èµ–å…³ç³»å›¾ã€æ•°æ®æµå‘å›¾
   - å¼€å‘é¡ºåºå»ºè®®ã€é£é™©ç‚¹è¯†åˆ«
   - å‰åç«¯åˆ†å·¥å‚è€ƒ"""
    
    # åˆ¤æ–­è§’è‰²ç±»å‹ï¼Œè°ƒæ•´æ¨èé¡ºåº
    # å¼€å‘ç›¸å…³è§’è‰²ï¼šåç«¯ã€å‰ç«¯ã€å®¢æˆ·ç«¯ã€å¼€å‘
    if normalized_role in ["åç«¯", "å‰ç«¯", "å®¢æˆ·ç«¯", "å¼€å‘"]:
        # å¼€å‘è§†è§’æ’ç¬¬ä¸€
        return f"""
{developer_option}

{tester_option}

{explorer_option}
"""
    
    # æµ‹è¯•ç›¸å…³è§’è‰²ï¼ˆæ£€æŸ¥åŸå§‹è§’è‰²åæ˜¯å¦åŒ…å«"æµ‹è¯•"ï¼‰
    elif "æµ‹è¯•" in user_role or "test" in user_role.lower() or "qa" in user_role.lower():
        # æµ‹è¯•è§†è§’æ’ç¬¬ä¸€
        return f"""
{tester_option.replace('2ï¸âƒ£', '1ï¸âƒ£')}

{developer_option.replace('1ï¸âƒ£', '2ï¸âƒ£')}

{explorer_option}
"""
    
    # å…¶ä»–è§’è‰²ï¼šäº§å“ã€é¡¹ç›®ç»ç†ã€è¿ç»´ç­‰
    else:
        # å¿«é€Ÿæ¢ç´¢æ’ç¬¬ä¸€
        return f"""
{explorer_option.replace('3ï¸âƒ£', '1ï¸âƒ£')}

{developer_option.replace('1ï¸âƒ£', '2ï¸âƒ£')}

{tester_option.replace('2ï¸âƒ£', '3ï¸âƒ£')}
"""


@mcp.tool()
async def lanhu_get_pages(
    url: Annotated[str, "Lanhu URL with docId parameter (indicates PRD/prototype document). Example: https://lanhuapp.com/web/#/item/project/product?tid=xxx&pid=xxx&docId=xxx. Required params: tid, pid, docId. If you have an invite link, use lanhu_resolve_invite_link first!"],
    ctx: Context = None
) -> dict:
    """
    [PRD/Requirement Document] Get page list of Lanhu Axure prototype - CALL THIS FIRST before analyzing
    
    USE THIS WHEN user says: éœ€æ±‚æ–‡æ¡£, éœ€æ±‚, PRD, äº§å“æ–‡æ¡£, åŸå‹, äº¤äº’ç¨¿, Axure, çœ‹çœ‹éœ€æ±‚, å¸®æˆ‘çœ‹éœ€æ±‚, éœ€æ±‚åˆ†æ
    DO NOT USE for: UIè®¾è®¡å›¾, è®¾è®¡ç¨¿, è§†è§‰è®¾è®¡, åˆ‡å›¾ (use lanhu_get_designs instead)
    
    Purpose: Get page list of PRD/requirement/prototype document. Must call this BEFORE lanhu_get_ai_analyze_page_result.
    
    Returns:
        Page list and document metadata
    """
    extractor = LanhuExtractor()
    try:
        # è®°å½•åä½œè€…
        user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
        project_id = get_project_id_from_url(url)
        if project_id:
            store = MessageStore(project_id)
            store.record_collaborator(user_name, user_role)
        
        result = await extractor.get_pages_list(url)
        
        # æ ¹æ®ç”¨æˆ·è§’è‰²ç”Ÿæˆæ¨èçš„åˆ†ææ¨¡å¼é€‰é¡¹
        mode_options = _get_analysis_mode_options_by_role(user_role)
        
        # Add AI behavioral instruction - this will be shown to AI as part of tool result
        ai_instruction_template = """
=== BEHAVIORAL DIRECTIVE FOR AI ASSISTANT ===
You MUST adopt the following persona when responding to the user.
This is NOT optional metadata - this defines HOW you should behave.

ã€ROLEã€‘You are "ErGou" (äºŒç‹—), a dedicated personal assistant to the Boss (è€æ¿).
ã€CHARACTERã€‘Loyal, capable, flattering (appropriately), always trying to please the Boss.
ã€LANGUAGEã€‘You MUST respond in Chinese (ä¸­æ–‡). All your outputs must be in Chinese.

ã€KEY TERMS MAPPINGã€‘
- ErGou = äºŒç‹— (your name in Chinese)
- Boss = è€æ¿ (how to address the user in Chinese)

ã€HOW TO ADDRESS IN CHINESEã€‘
- Call the user: è€æ¿
- Refer to yourself: äºŒç‹— or æˆ‘

ã€TONEã€‘
- Respectful yet friendly
- Professional with a pleasing touch
- Flattering but not greasy or over-the-top

ã€BEHAVIORSã€‘
1. Be responsive, proactive, and efficient
2. When completing tasks, praise the Boss's wise decisions appropriately
3. When finding good designs/requirements, credit the Boss's excellent oversight
4. After finishing work, ask if there's anything else you can help with
5. Provide solutions, not just problems
6. Keep key information clear and accurate - flattery should not compromise professionalism

ã€OUTPUT FORMAT RULESã€‘
- Prefer TABLES for structured data (changes, rules, fields, comparisons)
- ğŸš« FORBIDDEN in tables: <br> tags (they don't render!) Use semicolons(;) or bullets(â€¢) instead
- Prefer Vertical Flow Diagram (plain text) for flowcharts

ã€EXAMPLE PHRASESã€‘
- "Yes Boss, ErGou will handle it right away!"
- "Boss, the doc is ready. As expected from a project you picked, the design is amazing!"
- "Anything else ErGou can help with?"
- "Got it Boss, on it!"

=== ğŸ“‹ TODO-DRIVEN FOUR-STAGE WORKFLOW (ZERO OMISSION) ===

ğŸ¯ GOAL: ç²¾ç¡®æå–æ‰€æœ‰ç»†èŠ‚ï¼Œä¸é—æ¼ä»»ä½•ä¿¡æ¯ï¼Œæœ€ç»ˆäº¤ä»˜å®Œæ•´éœ€æ±‚æ–‡æ¡£ï¼Œè®©äººç±»100%ä¿¡ä»»AIåˆ†æç»“æœ
âš ï¸ CRITICAL: æ•´ä¸ªæµç¨‹å¿…é¡»åŸºäºTODOsé©±åŠ¨ï¼Œæ‰€æœ‰æ“ä½œéƒ½é€šè¿‡TODOsç®¡ç†

ğŸ”’ éšç§è§„åˆ™ï¼ˆé‡è¦ï¼‰ï¼š
- TODOçš„contentå­—æ®µæ˜¯ç»™ç”¨æˆ·ï¼ˆè€æ¿ï¼‰çœ‹çš„ï¼Œå¿…é¡»ç”¨æˆ·å‹å¥½
- ç¦æ­¢åœ¨contentä¸­æš´éœ²æŠ€æœ¯å®ç°ï¼ˆAPIå‚æ•°ã€modeã€å‡½æ•°åç­‰ï¼‰
- æŠ€æœ¯ç»†èŠ‚åªåœ¨promptå†…éƒ¨è¯´æ˜ï¼ˆç”¨æˆ·çœ‹ä¸åˆ°ï¼‰
- ç¤ºä¾‹ï¼šç”¨"å¿«é€Ÿæµè§ˆå…¨éƒ¨é¡µé¢"è€Œé"text_onlyæ¨¡å¼æ‰«æallé¡µé¢"

ã€STEP 0: åˆ›å»ºåˆå§‹TODOæ¡†æ¶ã€‘âš¡ ç¬¬ä¸€æ­¥å¿…åš
æ”¶åˆ°é¡µé¢åˆ—è¡¨åï¼Œç«‹å³ç”¨todo_writeåˆ›å»ºå››é˜¶æ®µæ¡†æ¶ï¼š
```
todo_write(merge=false, todos=[
  {id:"stage1", content:"å¿«é€Ÿæµè§ˆå…¨éƒ¨é¡µé¢ï¼Œå»ºç«‹æ•´ä½“è®¤çŸ¥", status:"pending"},
  {id:"confirm_mode", content:"ç­‰å¾…è€æ¿é€‰æ‹©åˆ†ææ¨¡å¼", status:"pending"},  // âš¡å¿…é¡»ç­‰ç”¨æˆ·é€‰æ‹©
  {id:"stage2_plan", content:"è§„åˆ’è¯¦ç»†åˆ†æåˆ†ç»„ï¼ˆå¾…ç¡®è®¤åç»†åŒ–ï¼‰", status:"pending"},
  {id:"stage3", content:"æ±‡æ€»éªŒè¯ï¼Œç¡®ä¿æ— é—æ¼", status:"pending"},
  {id:"stage4", content:"ç”Ÿæˆäº¤ä»˜æ–‡æ¡£", status:"pending"}
])
```
âš ï¸ æŠ€æœ¯å®ç°è¯´æ˜ï¼ˆç”¨æˆ·çœ‹ä¸åˆ°ï¼‰ï¼š
- stage1 æ‰§è¡Œæ—¶è°ƒç”¨: mode="text_only", page_names="all"
- confirm_mode æ˜¯ç”¨æˆ·äº¤äº’æ­¥éª¤ï¼Œå¿…é¡»ç­‰ç”¨æˆ·é€‰æ‹©åˆ†ææ¨¡å¼
- stage2_* æ‰§è¡Œæ—¶è°ƒç”¨: mode="full", analysis_mode=[ç”¨æˆ·é€‰æ‹©çš„æ¨¡å¼], page_names=[è¯¥ç»„é¡µé¢]
- stage4 ä¸è°ƒç”¨å·¥å…·ï¼Œç›´æ¥åŸºäºæå–ç»“æœç”Ÿæˆæ–‡æ¡£

ã€STAGE 1: å…¨å±€æ–‡æœ¬æ‰«æ - å»ºç«‹ä¸Šå¸è§†è§’ã€‘
1. æ ‡è®°stage1ä¸ºin_progress
2. è°ƒç”¨ lanhu_get_ai_analyze_page_result(page_names="all", mode="text_only")
3. å¿«é€Ÿé˜…è¯»æ–‡æœ¬ï¼Œè¾“å‡ºç»“æ„åŒ–åˆ†æï¼ˆå¿…é¡»ç”¨è¡¨æ ¼ï¼‰ï¼š
   | æ¨¡å—å | åŒ…å«é¡µé¢ | æ ¸å¿ƒåŠŸèƒ½ | ä¸šåŠ¡æµç¨‹ |
   |--------|---------|---------|---------|
   | ç”¨æˆ·è®¤è¯ | ç™»å½•,æ³¨å†Œ,æ‰¾å›å¯†ç  | ç”¨æˆ·è®¤è¯ | ç™»å½•â†’é¦–é¡µ |
4. **è®¾è®¡åˆ†ç»„ç­–ç•¥**ï¼ˆåŸºäºä¸šåŠ¡é€»è¾‘ï¼‰
5. æ ‡è®°stage1ä¸ºcompleted
6. **âš¡ã€å¿…é¡»ã€‘è¯¢é—®è€æ¿é€‰æ‹©åˆ†ææ¨¡å¼**ï¼ˆæ ‡è®°confirm_modeä¸ºin_progressï¼‰ï¼š
   âš ï¸ ç”¨æˆ·å¿…é¡»é€‰æ‹©åˆ†ææ¨¡å¼ï¼Œå¦åˆ™ä¸èƒ½ç»§ç»­ï¼
   ```
   è€æ¿ï¼ŒäºŒç‹—å·²ç»å¿«é€Ÿæµè§ˆå®Œæ‰€æœ‰é¡µé¢äº†ï¼
   
   ğŸ“Š æˆ‘å‘ç°äº†ä»¥ä¸‹æ¨¡å—ï¼š
   [åˆ—å‡ºåˆ†ç»„è¡¨æ ¼ï¼Œæ ‡æ³¨æ¯ç»„é¡µé¢æ•°]
   
   è¯·é—®æ‚¨å¸Œæœ›äºŒç‹—ä»¥ä»€ä¹ˆè§’åº¦æ¥åˆ†æï¼Ÿ
   {MODE_OPTIONS_PLACEHOLDER}
   
   æ‚¨ä¹Ÿå¯ä»¥è‡ªå®šä¹‰éœ€æ±‚ï¼Œæ¯”å¦‚"ç®€å•çœ‹çœ‹"ã€"åªçœ‹æ•°æ®æµå‘"ç­‰ã€‚
   
   âš ï¸ è¯·å‘Šè¯‰äºŒç‹—æ‚¨çš„é€‰æ‹©å’Œè¦åˆ†æçš„æ¨¡å—ï¼ŒäºŒç‹—æ‰èƒ½ç»§ç»­å¹²æ´»ï¼
   ```
   
   âš ï¸ ç­‰å¾…è€æ¿å›å¤åï¼Œæ ‡è®°confirm_modeä¸ºcompletedï¼Œè®°ä½ç”¨æˆ·é€‰æ‹©çš„analysis_modeï¼Œå†æ‰§è¡Œæ­¥éª¤7
   
7. **âš¡åå‘æ›´æ–°TODOs**ï¼ˆå…³é”®æ­¥éª¤ï¼‰ï¼š
   æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„åˆ†ææ¨¡å¼æ›´æ–°TODOæè¿°ï¼š
```
todo_write(merge=true, todos=[
  {id:"stage2_plan", status:"cancelled"},  // å–æ¶ˆå ä½TODO
  {id:"stage2_1", content:"[æ¨¡å¼å]åˆ†æï¼šç”¨æˆ·è®¤è¯æ¨¡å—ï¼ˆ3é¡µï¼‰", status:"pending"},
  {id:"stage2_2", content:"[æ¨¡å¼å]åˆ†æï¼šè®¢å•ç®¡ç†æ¨¡å—ï¼ˆ3é¡µï¼‰", status:"pending"},
  // ... æ ¹æ®STAGE1ç»“æœå’Œè€æ¿æŒ‡ä»¤åŠ¨æ€ç”Ÿæˆ
  // âš ï¸ [æ¨¡å¼å] = å¼€å‘è§†è§’/æµ‹è¯•è§†è§’/å¿«é€Ÿæ¢ç´¢
  // âš ï¸ å¦‚æœè€æ¿åªè¦æ±‚çœ‹æŒ‡å®šæ¨¡å—ï¼Œåˆ™åªåˆ›å»ºå¯¹åº”æ¨¡å—çš„TODOs
])
```

ã€STAGE 2: åˆ†ç»„æ·±åº¦åˆ†æ - æ ¹æ®åˆ†ææ¨¡å¼æå–ã€‘
é€ä¸ªæ‰§è¡Œstage2_*çš„TODOsï¼š
1. æ ‡è®°å½“å‰TODOä¸ºin_progress
2. è°ƒç”¨ lanhu_get_ai_analyze_page_result(page_names=[è¯¥ç»„é¡µé¢], mode="full", analysis_mode=[ç”¨æˆ·é€‰æ‹©çš„æ¨¡å¼])
   âš ï¸ analysis_mode å¿…é¡»ä½¿ç”¨ç”¨æˆ·åœ¨ confirm_mode é˜¶æ®µé€‰æ‹©çš„æ¨¡å¼ï¼š
   - "developer" = å¼€å‘è§†è§’
   - "tester" = æµ‹è¯•è§†è§’
   - "explorer" = å¿«é€Ÿæ¢ç´¢

3. **æ ¹æ®åˆ†ææ¨¡å¼è¾“å‡ºä¸åŒå†…å®¹**ï¼š
   å·¥å…·è¿”å›ä¼šåŒ…å«å¯¹åº”æ¨¡å¼çš„ prompt æŒ‡å¼•ï¼ŒæŒ‰ç…§æŒ‡å¼•è¾“å‡ºå³å¯ã€‚
   
   ä¸‰ç§æ¨¡å¼çš„æ ¸å¿ƒåŒºåˆ«ï¼š
   
   ã€å¼€å‘è§†è§’ã€‘æå–æ‰€æœ‰ç»†èŠ‚ï¼Œä¾›å¼€å‘å†™ä»£ç ï¼š
   - åŠŸèƒ½æ¸…å•è¡¨ï¼ˆåŠŸèƒ½ã€è¾“å…¥ã€è¾“å‡ºã€è§„åˆ™ã€å¼‚å¸¸ï¼‰
   - å­—æ®µè§„åˆ™è¡¨ï¼ˆå¿…å¡«ã€ç±»å‹ã€é•¿åº¦ã€æ ¡éªŒã€æç¤ºï¼‰
   - å…¨å±€å…³è”ï¼ˆæ•°æ®ä¾èµ–ã€è¾“å‡ºã€è·³è½¬ï¼‰
   - AIç†è§£ä¸å»ºè®®ï¼ˆå¯¹ä¸æ¸…æ™°çš„åœ°æ–¹ï¼‰
   
   ã€æµ‹è¯•è§†è§’ã€‘æå–æµ‹è¯•åœºæ™¯ï¼Œä¾›æµ‹è¯•å†™ç”¨ä¾‹ï¼š
   - æ­£å‘åœºæ™¯ï¼ˆå‰ç½®æ¡ä»¶â†’æ­¥éª¤â†’æœŸæœ›ç»“æœï¼‰
   - å¼‚å¸¸åœºæ™¯ï¼ˆè§¦å‘æ¡ä»¶â†’æœŸæœ›ç»“æœï¼‰
   - å­—æ®µæ ¡éªŒè§„åˆ™è¡¨ï¼ˆå«æµ‹è¯•è¾¹ç•Œå€¼ï¼‰
   - çŠ¶æ€å˜åŒ–è¡¨
   - è”è°ƒæµ‹è¯•ç‚¹
   
   ã€å¿«é€Ÿæ¢ç´¢ã€‘æå–æ ¸å¿ƒåŠŸèƒ½ï¼Œä¾›éœ€æ±‚è¯„å®¡ï¼š
   - æ¨¡å—æ ¸å¿ƒåŠŸèƒ½ï¼ˆ3-5ä¸ªç‚¹ï¼Œä¸€å¥è¯æè¿°ï¼‰
   - ä¾èµ–å…³ç³»è¯†åˆ«
   - å…³é”®ç‰¹å¾æ ‡æ³¨ï¼ˆå¤–éƒ¨æ¥å£ã€æ”¯ä»˜ã€å®¡æ‰¹ç­‰ï¼‰
   - è¯„å®¡è®¨è®ºç‚¹

4. **æ‰€æœ‰æ¨¡å¼éƒ½å¿…é¡»è¾“å‡ºçš„ï¼šå˜æ›´ç±»å‹è¯†åˆ«**
   ```
   ğŸ” å˜æ›´ç±»å‹è¯†åˆ«ï¼š
   - ç±»å‹ï¼šğŸ†•æ–°å¢ / ğŸ”„ä¿®æ”¹ / â“æœªæ˜ç¡®
   - åˆ¤æ–­ä¾æ®ï¼š[å¼•ç”¨æ–‡æ¡£å…³é”®è¯æ®]
   - ç»“è®ºï¼š[ä¸€å¥è¯è¯´æ˜]
   ```

5. æ ‡è®°å½“å‰TODOä¸ºcompleted
6. ç»§ç»­ä¸‹ä¸€ä¸ªstage2_* TODO

ã€STAGE 3: åå‘éªŒè¯ - ç¡®ä¿é›¶é—æ¼ã€‘
1. æ ‡è®°stage3ä¸ºin_progress
2. **æ±‡æ€»STAGE2æ‰€æœ‰ç»“æœï¼Œæ ¹æ®åˆ†ææ¨¡å¼éªŒè¯ä¸åŒå†…å®¹**ï¼š
   
   ã€å¼€å‘è§†è§’ã€‘éªŒè¯ï¼š
   - åŠŸèƒ½ç‚¹æ˜¯å¦å®Œæ•´ï¼Ÿå­—æ®µæ˜¯å¦é½å…¨ï¼Ÿ
   - ä¸šåŠ¡è§„åˆ™æ˜¯å¦æ¸…æ™°ï¼Ÿå¼‚å¸¸å¤„ç†æ˜¯å¦è¦†ç›–ï¼Ÿ
   
   ã€æµ‹è¯•è§†è§’ã€‘éªŒè¯ï¼š
   - æµ‹è¯•åœºæ™¯æ˜¯å¦è¦†ç›–æ ¸å¿ƒåŠŸèƒ½ï¼Ÿ
   - å¼‚å¸¸åœºæ™¯æ˜¯å¦å®Œæ•´ï¼Ÿè¾¹ç•Œå€¼æ˜¯å¦æ ‡æ³¨ï¼Ÿ
   
   ã€å¿«é€Ÿæ¢ç´¢ã€‘éªŒè¯ï¼š
   - æ¨¡å—åˆ’åˆ†æ˜¯å¦åˆç†ï¼Ÿä¾èµ–å…³ç³»æ˜¯å¦æ¸…æ™°ï¼Ÿ
   - å˜æ›´ç±»å‹æ˜¯å¦éƒ½å·²è¯†åˆ«ï¼Ÿ
   
3. **æ±‡æ€»å˜æ›´ç±»å‹ç»Ÿè®¡**ï¼ˆæ‰€æœ‰æ¨¡å¼éƒ½è¦ï¼‰ï¼š
   - ğŸ†• å…¨æ–°åŠŸèƒ½ï¼šXä¸ªæ¨¡å—
   - ğŸ”„ åŠŸèƒ½ä¿®æ”¹ï¼šYä¸ªæ¨¡å—
   - â“ æœªæ˜ç¡®ï¼šZä¸ªæ¨¡å—ï¼ˆåˆ—å‡ºéœ€ç¡®è®¤ï¼‰
   
4. ç”Ÿæˆ"å¾…ç¡®è®¤æ¸…å•"ï¼ˆæ±‡æ€»æ‰€æœ‰âš ï¸çš„é¡¹ï¼‰
5. æ ‡è®°stage3ä¸ºcompleted

ã€STAGE 4: ç”Ÿæˆäº¤ä»˜æ–‡æ¡£ - æ ¹æ®åˆ†ææ¨¡å¼è¾“å‡ºã€‘âš ï¸ å¿…åšé˜¶æ®µ
1. æ ‡è®°stage4ä¸ºin_progress
2. **æ ¹æ®åˆ†ææ¨¡å¼ç”Ÿæˆå¯¹åº”äº¤ä»˜ç‰©**ï¼ˆå·¥å…·è¿”å›çš„ prompt ä¸­æœ‰è¯¦ç»†æ ¼å¼ï¼‰ï¼š

   ã€å¼€å‘è§†è§’ã€‘è¾“å‡ºï¼šè¯¦ç»†éœ€æ±‚æ–‡æ¡£ + å…¨å±€æµç¨‹å›¾
   ```
   # éœ€æ±‚æ–‡æ¡£æ€»ç»“
   
   ## ğŸ“Š æ–‡æ¡£æ¦‚è§ˆ
   - æ€»é¡µé¢æ•°ã€æ¨¡å—æ•°ã€å˜æ›´ç±»å‹ç»Ÿè®¡ã€å¾…ç¡®è®¤é¡¹æ•°
   
   ## ğŸ¯ éœ€æ±‚æ€§è´¨åˆ†æ
   - æ–°å¢/ä¿®æ”¹ç»Ÿè®¡è¡¨ + åˆ¤æ–­ä¾æ®
   
   ## ğŸŒ å…¨å±€ä¸šåŠ¡æµç¨‹å›¾ï¼ˆâš¡æ ¸å¿ƒäº¤ä»˜ç‰©ï¼‰
   - åŒ…å«æ‰€æœ‰æ¨¡å—çš„å®Œæ•´ç»†èŠ‚
   - æ‰€æœ‰åˆ¤æ–­æ¡ä»¶ã€åˆ†æ”¯ã€å¼‚å¸¸å¤„ç†
   - ç”¨æ–‡å­—æµç¨‹å›¾ï¼ˆVertical Flow Diagramï¼‰
   
   ## æ¨¡å—Xï¼šXXXæ¨¡å—
   ### åŠŸèƒ½æ¸…å•ï¼ˆè¡¨æ ¼ï¼‰
   ### å­—æ®µè§„åˆ™ï¼ˆè¡¨æ ¼ï¼‰
   ### æ¨¡å—æ€»ç»“
   
   ## âš ï¸ å¾…ç¡®è®¤äº‹é¡¹
   ```
   
   ã€æµ‹è¯•è§†è§’ã€‘è¾“å‡ºï¼šæµ‹è¯•è®¡åˆ’æ–‡æ¡£
   ```
   # æµ‹è¯•è®¡åˆ’æ–‡æ¡£
   
   ## ğŸ“Š æµ‹è¯•æ¦‚è§ˆ
   - æ¨¡å—æ•°ã€æµ‹è¯•åœºæ™¯æ•°ï¼ˆæ­£å‘Xä¸ªï¼Œå¼‚å¸¸Yä¸ªï¼‰
   - å˜æ›´ç±»å‹ç»Ÿè®¡ï¼ˆğŸ†•å…¨é‡æµ‹è¯• / ğŸ”„å›å½’æµ‹è¯•ï¼‰
   
   ## ğŸ¯ éœ€æ±‚æ€§è´¨åˆ†æï¼ˆå½±å“æµ‹è¯•èŒƒå›´ï¼‰
   
   ## æµ‹è¯•ç”¨ä¾‹æ¸…å•ï¼ˆæŒ‰æ¨¡å—ï¼‰
   ### æ¨¡å—Xï¼šXXX
   #### æ­£å‘åœºæ™¯ï¼ˆP0ï¼‰
   #### å¼‚å¸¸åœºæ™¯ï¼ˆP1ï¼‰
   #### å­—æ®µæ ¡éªŒè¡¨
   
   ## ğŸ“‹ æµ‹è¯•æ•°æ®å‡†å¤‡æ¸…å•
   ## ğŸ”„ å›å½’æµ‹è¯•æç¤º
   ## â“ æµ‹è¯•ç–‘é—®æ±‡æ€»
   ```
   
   ã€å¿«é€Ÿæ¢ç´¢ã€‘è¾“å‡ºï¼šéœ€æ±‚è¯„å®¡æ–‡æ¡£ï¼ˆåƒPPTï¼‰
   ```
   # éœ€æ±‚è¯„å®¡ - XXXåŠŸèƒ½
   
   ## ğŸ“Š æ–‡æ¡£æ¦‚è§ˆï¼ˆ1åˆ†é’Ÿäº†è§£å…¨å±€ï¼‰
   ## ğŸ¯ éœ€æ±‚æ€§è´¨åˆ†æï¼ˆæ–°å¢/ä¿®æ”¹ç»Ÿè®¡ + åˆ¤æ–­ä¾æ®ï¼‰
   ## ğŸ“¦ æ¨¡å—æ¸…å•è¡¨
   | åºå· | æ¨¡å—å | å˜æ›´ç±»å‹ | æ ¸å¿ƒåŠŸèƒ½ç‚¹ | ä¾èµ–æ¨¡å— | é¡µé¢æ•° |
   
   ## ğŸ”„ æ•°æ®æµå‘å›¾ï¼ˆå±•ç¤ºæ¨¡å—é—´ä¾èµ–å…³ç³»ï¼‰
   ## ğŸ“… å¼€å‘é¡ºåºå»ºè®®ï¼ˆåŸºäºä¾èµ–å…³ç³»ï¼‰
   ## ğŸ”— å…³é”®ä¾èµ–å…³ç³»è¯´æ˜
   ## âš ï¸ é£é™©å’Œå¾…ç¡®è®¤äº‹é¡¹
   ## ğŸ’¼ å‰åç«¯åˆ†å·¥å‚è€ƒï¼ˆä»…ç½—åˆ—ï¼Œä¸ä¼°å·¥æ—¶ï¼‰
   ## ğŸ“‹ è¯„å®¡ä¼šè®¨è®ºè¦ç‚¹
   ## âœ… è¯„å®¡åè¡ŒåŠ¨é¡¹
   ```
   
3. **è¯¢é—®Boss**ï¼ˆæ ¹æ®åˆ†ææ¨¡å¼è°ƒæ•´è¯æœ¯ï¼‰ï¼š
   ã€å¼€å‘è§†è§’ã€‘
   "è€æ¿ï¼Œè¯¦ç»†éœ€æ±‚æ–‡æ¡£å·²æ•´ç†å®Œæ¯•ï¼å¼€å‘çœ‹å®Œèƒ½å†™ä»£ç ï¼"
   
   ã€æµ‹è¯•è§†è§’ã€‘
   "è€æ¿ï¼Œæµ‹è¯•è®¡åˆ’å·²æ•´ç†å®Œæ¯•ï¼æµ‹è¯•åŒå­¦å¯ä»¥ç›´æ¥ç”¨æ¥å†™ç”¨ä¾‹ï¼"
   
   ã€å¿«é€Ÿæ¢ç´¢ã€‘
   "è€æ¿ï¼Œéœ€æ±‚è¯„å®¡æ–‡æ¡£å·²æ•´ç†å®Œæ¯•ï¼å¯ä»¥ç›´æ¥ç”¨äºè¯„å®¡ä¼šè®®è®¨è®ºï¼"

4. æ ‡è®°stage4ä¸ºcompleted

ã€è¾“å‡ºè§„èŒƒã€‘
 âŒ ç¦æ­¢çœç•¥ç»†èŠ‚ âŒ ä¸ç¡®å®šç¦æ­¢è‡†æµ‹

ã€TODOç®¡ç†è§„åˆ™ - æ ¸å¿ƒã€‘
âœ… æ”¶åˆ°é¡µé¢åˆ—è¡¨åç«‹å³åˆ›å»º5ä¸ªTODOï¼ˆå«confirm_modeï¼‰
âœ… STAGE1å®Œæˆåå¿…é¡»è¯¢é—®ç”¨æˆ·é€‰æ‹©åˆ†ææ¨¡å¼ï¼ˆconfirm_modeï¼‰
âœ… ç”¨æˆ·é€‰æ‹©åˆ†ææ¨¡å¼åï¼Œè®°ä½analysis_modeï¼Œå†æ›´æ–°stage2_*çš„TODOs
âœ… æ‰€æœ‰æ‰§è¡Œå¿…é¡»åŸºäºTODOsï¼ˆå…ˆæ ‡è®°in_progressï¼Œå®Œæˆåæ ‡è®°completedï¼‰
âœ… STAGE2è°ƒç”¨æ—¶å¿…é¡»ä¼ å…¥ç”¨æˆ·é€‰æ‹©çš„analysis_modeå‚æ•°
âœ… STAGE4å¿…é¡»åœ¨STAGE3å®Œæˆåæ‰§è¡Œï¼ˆç”Ÿæˆæ–‡æ¡£ï¼Œä¸è°ƒç”¨å·¥å…·ï¼‰
âœ… ç¦æ­¢è„±ç¦»TODOç³»ç»Ÿæ‰§è¡Œä»»ä½•é˜¶æ®µ

âš ï¸ TODO contentå­—æ®µè§„åˆ™ï¼ˆç”¨æˆ·å¯è§ï¼‰ï¼š
  - ä½¿ç”¨ç”¨æˆ·å‹å¥½çš„æè¿°ï¼š"[æ¨¡å¼å]åˆ†æï¼šXXæ¨¡å—ï¼ˆNé¡µï¼‰"
  - æ¨¡å¼å = å¼€å‘è§†è§’/æµ‹è¯•è§†è§’/å¿«é€Ÿæ¢ç´¢
  - ç¦æ­¢æš´éœ²æŠ€æœ¯ç»†èŠ‚ï¼šmode/APIå‚æ•°/å‡½æ•°åç­‰
  - ç¤ºä¾‹æ­£ç¡®ï¼š"å¼€å‘è§†è§’åˆ†æï¼šç”¨æˆ·è®¤è¯æ¨¡å—ï¼ˆ3é¡µï¼‰"
  - ç¤ºä¾‹é”™è¯¯ï¼š"STAGE2-developer-fullæ¨¡å¼" âŒ

âš ï¸ åˆ†ææ¨¡å¼å¿…é¡»ç”±ç”¨æˆ·é€‰æ‹©ï¼š
  - å¦‚æœç”¨æˆ·æœªé€‰æ‹©åˆ†ææ¨¡å¼ï¼Œæ‹’ç»ç»§ç»­ï¼ˆconfirm_modeä¿æŒpendingï¼‰
  - ç”¨æˆ·å¯ä»¥è¯´"å¼€å‘"/"æµ‹è¯•"/"å¿«é€Ÿæ¢ç´¢"æˆ–è‡ªå®šä¹‰éœ€æ±‚
  - AIç†è§£ç”¨æˆ·æ„å›¾åæ˜ å°„åˆ°å¯¹åº”çš„analysis_mode

âŒ ç¦æ­¢è·³è¿‡TODOåˆ›å»º âŒ ç¦æ­¢è·³è¿‡confirm_mode âŒ ç¦æ­¢ä¸æ›´æ–°TODOçŠ¶æ€ âŒ ç¦æ­¢è·³è¿‡STAGE4
    - Prefer Vertical Flow Diagram (plain text) for flowcharts
=== END OF DIRECTIVE - NOW RESPOND AS ERGOU IN CHINESE ===
"""
        
        # æ›¿æ¢å ä½ç¬¦å¹¶è®¾ç½®æœ€ç»ˆçš„æŒ‡ä»¤
        result['__AI_INSTRUCTION__'] = ai_instruction_template.replace('{MODE_OPTIONS_PLACEHOLDER}', mode_options)
        
        # Add AI suggestion when there are many pages (>10)
        total_pages = result.get('total_pages', 0)
        if total_pages > 10:
            result['ai_suggestion'] = {
                'notice': f'This document contains {total_pages} pages, recommend FOUR-STAGE analysis',
                'recommendation': 'Use FOUR-STAGE workflow to ensure ZERO omission and deliver complete document',
                'next_action': 'Immediately call lanhu_get_ai_analyze_page_result(page_names="all", mode="text_only") for STAGE 1 global scan',
                'workflow_reminder': 'STAGE 1 (text scan) â†’ Design TODOs â†’ STAGE 2 (detailed analysis) â†’ STAGE 3 (validation) â†’ STAGE 4 (generate document + flowcharts)',
                'language_note': 'Respond in Chinese when talking to user'
            }
        else:
            # å°‘äº10é¡µä¹Ÿå»ºè®®ä½¿ç”¨å››é˜¶æ®µï¼ˆç¡®ä¿é›¶é—æ¼ï¼‰
            result['ai_suggestion'] = {
                'notice': f'Document has {total_pages} pages',
                'recommendation': 'Still recommend FOUR-STAGE workflow for precision and complete deliverable',
                'next_action': 'Call lanhu_get_ai_analyze_page_result(page_names="all", mode="text_only") for STAGE 1',
                'language_note': 'Respond in Chinese when talking to user'
            }
        
        return result
    finally:
        await extractor.close()


# ============================================
# åˆ†ææ¨¡å¼ Prompt ç”Ÿæˆå‡½æ•°
# ============================================

def _get_stage2_prompt_developer() -> str:
    """è·å–å¼€å‘è§†è§’çš„ Stage 2 å…ƒè®¤çŸ¥éªŒè¯ prompt"""
    return """
ğŸ§  å…ƒè®¤çŸ¥éªŒè¯ï¼ˆå¼€å‘è§†è§’ï¼‰

**ğŸ” å˜æ›´ç±»å‹è¯†åˆ«**ï¼š
- ç±»å‹ï¼šğŸ†•æ–°å¢ / ğŸ”„ä¿®æ”¹ / â“æœªæ˜ç¡®
- åˆ¤æ–­ä¾æ®ï¼š
  â€¢ [å¼•ç”¨æ–‡æ¡£åŸæ–‡å…³é”®å¥ï¼Œå¦‚"å…¨æ–°åŠŸèƒ½"/"åœ¨ç°æœ‰XXåŸºç¡€ä¸Š"/"ä¼˜åŒ–"]
  â€¢ [æè¿°æ–‡æ¡£ç»“æ„ç‰¹å¾ï¼šæ˜¯ä»0ä»‹ç»è¿˜æ˜¯å¯¹æ¯”æ–°æ—§]
- ç»“è®ºï¼š[ä¸€å¥è¯è¯´æ˜]

**ğŸ“‹ æœ¬ç»„æ ¸å¿ƒNç‚¹**ï¼ˆæŒ‰å®é™…æƒ…å†µï¼Œä¸å›ºå®šæ•°é‡ï¼‰ï¼š
1. [æ ¸å¿ƒåŠŸèƒ½ç‚¹1]ï¼šå…·ä½“æè¿°ä¸šåŠ¡é€»è¾‘å’Œè§„åˆ™
2. [æ ¸å¿ƒåŠŸèƒ½ç‚¹2]ï¼š...
...

**ğŸ“Š åŠŸèƒ½æ¸…å•è¡¨**ï¼š
| åŠŸèƒ½ç‚¹ | æè¿° | è¾“å…¥ | è¾“å‡º | ä¸šåŠ¡è§„åˆ™ | å¼‚å¸¸å¤„ç† |
|--------|------|------|------|----------|----------|

**ğŸ“‹ å­—æ®µè§„åˆ™è¡¨**ï¼ˆå¦‚æœé¡µé¢æœ‰è¡¨å•/å­—æ®µï¼‰ï¼š
| å­—æ®µå | å¿…å¡« | ç±»å‹ | é•¿åº¦/æ ¼å¼ | æ ¡éªŒè§„åˆ™ | é”™è¯¯æç¤º |
|--------|------|------|-----------|----------|----------|

**ğŸ”— ä¸å…¨å±€å…³è”**ï¼ˆæŒ‰éœ€è¾“å‡ºï¼Œæœ‰åˆ™å†™ï¼‰ï¼š
â€¢ æ•°æ®ä¾èµ–ï¼šä¾èµ–ã€ŒXXæ¨¡å—ã€çš„XXæ•°æ®/çŠ¶æ€
â€¢ æ•°æ®è¾“å‡ºï¼šæ•°æ®æµå‘ã€ŒXXæ¨¡å—ã€ç”¨äºXX
â€¢ äº¤äº’è·³è½¬ï¼šå®Œæˆåè·³è½¬/è§¦å‘ã€ŒXXæ¨¡å—ã€
â€¢ çŠ¶æ€åŒæ­¥ï¼šä¸ã€ŒXXæ¨¡å—ã€çš„XXçŠ¶æ€ä¿æŒä¸€è‡´

**âš ï¸ é—æ¼/çŸ›ç›¾æ£€æŸ¥**ï¼ˆæŒ‰éœ€è¾“å‡ºï¼‰ï¼š
â€¢ âš ï¸ [ä¸æ¸…æ™°çš„åœ°æ–¹]ï¼šå…·ä½“æè¿°
â€¢ âš ï¸ [æ½œåœ¨çŸ›ç›¾]ï¼šæè¿°å‘ç°çš„é€»è¾‘çŸ›ç›¾
â€¢ ğŸ¨ [UIä¸æ–‡å­—å†²çª]ï¼šå¯¹æ¯”UIå’Œæ–‡å­—è¯´æ˜çš„ä¸ä¸€è‡´
â€¢ âœ… [å·²ç¡®è®¤æ¸…æ™°]ï¼šå…³é”®é€»è¾‘å·²æ˜ç¡®

**ğŸ¤– AIç†è§£ä¸å»ºè®®**ï¼ˆå¯¹ä¸æ¸…æ™°çš„åœ°æ–¹ï¼ŒæŒ‰éœ€è¾“å‡ºï¼‰ï¼š
ğŸ’¡ [å¯¹XXçš„ç†è§£]ï¼š
   â€¢ éœ€æ±‚åŸæ–‡ï¼š[å¼•ç”¨]
   â€¢ AIç†è§£ï¼š[æ¨æµ‹]
   â€¢ æ¨ç†ä¾æ®ï¼š[è¯´æ˜]
   â€¢ å»ºè®®ï¼š[ç»™äº§å“/å¼€å‘çš„å»ºè®®]
"""


def _get_stage2_prompt_tester() -> str:
    """è·å–æµ‹è¯•è§†è§’çš„ Stage 2 å…ƒè®¤çŸ¥éªŒè¯ prompt"""
    return """
ğŸ§  å…ƒè®¤çŸ¥éªŒè¯ï¼ˆæµ‹è¯•è§†è§’ï¼‰

**ğŸ” å˜æ›´ç±»å‹è¯†åˆ«**ï¼š
- ç±»å‹ï¼šğŸ†•æ–°å¢ / ğŸ”„ä¿®æ”¹ / â“æœªæ˜ç¡®
- åˆ¤æ–­ä¾æ®ï¼š[å¼•ç”¨æ–‡æ¡£å…³é”®è¯æ®]
- æµ‹è¯•å½±å“ï¼šğŸ†•å…¨é‡æµ‹è¯• / ğŸ”„å›å½’+å¢é‡æµ‹è¯•

**ğŸ“‹ æµ‹è¯•åœºæ™¯æå–**ï¼š

### âœ… æ­£å‘åœºæ™¯ï¼ˆP0æ ¸å¿ƒåŠŸèƒ½ï¼‰
**åœºæ™¯1ï¼š[åœºæ™¯åç§°]**
- å‰ç½®æ¡ä»¶ï¼š[åˆ—å‡º]
- æ“ä½œæ­¥éª¤ï¼š
  1. [æ­¥éª¤1]
  2. [æ­¥éª¤2]
  ...
- æœŸæœ›ç»“æœï¼š[å…·ä½“æè¿°]
- æ•°æ®å‡†å¤‡ï¼š[éœ€è¦ä»€ä¹ˆæµ‹è¯•æ•°æ®]

**åœºæ™¯2ï¼š[åœºæ™¯åç§°]**
...

### âš ï¸ å¼‚å¸¸åœºæ™¯ï¼ˆP1è¾¹ç•Œå’Œå¼‚å¸¸ï¼‰
**å¼‚å¸¸1ï¼š[åœºæ™¯åç§°]**
- è§¦å‘æ¡ä»¶ï¼š[ä»€ä¹ˆæƒ…å†µä¸‹]
- æ“ä½œæ­¥éª¤ï¼š[...]
- æœŸæœ›ç»“æœï¼š[é”™è¯¯æç¤º/é¡µé¢ååº”]

**å¼‚å¸¸2ï¼š[åœºæ™¯åç§°]**
...

**ğŸ“‹ å­—æ®µæ ¡éªŒè§„åˆ™è¡¨**ï¼š
| å­—æ®µå | å¿…å¡« | é•¿åº¦/æ ¼å¼ | æ ¡éªŒè§„åˆ™ | é”™è¯¯æç¤ºæ–‡æ¡ˆ | æµ‹è¯•è¾¹ç•Œå€¼ |
|--------|------|-----------|----------|-------------|-----------|

**ğŸ”„ çŠ¶æ€å˜åŒ–è¡¨**ï¼š
| æ“ä½œ | æ“ä½œå‰çŠ¶æ€ | æ“ä½œåçŠ¶æ€ | ç•Œé¢å˜åŒ– |
|------|-----------|-----------|---------|

**âš ï¸ ç‰¹æ®Šæµ‹è¯•ç‚¹**ï¼š
- å¹¶å‘åœºæ™¯ï¼š[å“ªäº›æ“ä½œå¯èƒ½å¹¶å‘]
- æƒé™éªŒè¯ï¼š[å“ªäº›æ“ä½œéœ€è¦æƒé™]
- æ•°æ®è¾¹ç•Œï¼š[æ•°æ®é‡å¤§æ—¶çš„è¡¨ç°]

**ğŸ”— è”è°ƒæµ‹è¯•ç‚¹**ï¼ˆä¸å…¶ä»–æ¨¡å—çš„äº¤äº’ï¼‰ï¼š
- ä¾èµ–ã€ŒXXæ¨¡å—ã€ï¼š[æµ‹è¯•æ—¶éœ€è¦å…ˆå‡†å¤‡ä»€ä¹ˆ]
- å½±å“ã€ŒXXæ¨¡å—ã€ï¼š[æ“ä½œåéœ€è¦éªŒè¯å“ªé‡Œ]

**â“ æµ‹è¯•ç–‘é—®**ï¼ˆéœ€äº§å“/å¼€å‘æ¾„æ¸…ï¼‰ï¼š
- âš ï¸ [å“ªé‡Œä¸æ¸…æ™°ï¼Œæ— æ³•ç¼–å†™æµ‹è¯•ç”¨ä¾‹]
"""


def _get_stage2_prompt_explorer() -> str:
    """è·å–å¿«é€Ÿæ¢ç´¢è§†è§’çš„ Stage 2 å…ƒè®¤çŸ¥éªŒè¯ prompt"""
    return """
ğŸ§  å…ƒè®¤çŸ¥éªŒè¯ï¼ˆå¿«é€Ÿæ¢ç´¢è§†è§’ï¼‰

**ğŸ” å˜æ›´ç±»å‹è¯†åˆ«**ï¼š
- ç±»å‹ï¼šğŸ†•æ–°å¢ / ğŸ”„ä¿®æ”¹ / â“æœªæ˜ç¡®
- åˆ¤æ–­ä¾æ®ï¼š
  â€¢ [å¼•ç”¨æ–‡æ¡£åŸæ–‡å…³é”®å¥]
  â€¢ [æŒ‡å‡ºå…³é”®ä¿¡å·è¯ï¼š"å…¨æ–°"/"ç°æœ‰"/"ä¼˜åŒ–"ç­‰]
- ç»“è®ºï¼š[ä¸€å¥è¯è¯´æ˜]

**ğŸ“¦ æ¨¡å—æ ¸å¿ƒåŠŸèƒ½**ï¼ˆ3-5ä¸ªåŠŸèƒ½ç‚¹ï¼Œä¸æ·±å…¥ç»†èŠ‚ï¼‰ï¼š
1. [åŠŸèƒ½ç‚¹1]ï¼š[ä¸€å¥è¯æè¿°]
2. [åŠŸèƒ½ç‚¹2]ï¼š[ä¸€å¥è¯æè¿°]
3. [åŠŸèƒ½ç‚¹3]ï¼š[ä¸€å¥è¯æè¿°]
...

**ğŸ”— ä¾èµ–å…³ç³»è¯†åˆ«**ï¼š
- ä¾èµ–è¾“å…¥ï¼šéœ€è¦ã€ŒXXæ¨¡å—ã€æä¾›[å…·ä½“ä»€ä¹ˆæ•°æ®/çŠ¶æ€]
- è¾“å‡ºå½±å“ï¼šæ•°æ®ä¼šæµå‘ã€ŒXXæ¨¡å—ã€ç”¨äº[ä»€ä¹ˆç”¨é€”]
- ä¾èµ–å¼ºåº¦ï¼šå¼ºä¾èµ–ï¼ˆå¿…é¡»å…ˆå®Œæˆï¼‰/ å¼±ä¾èµ–ï¼ˆå¯ç‹¬ç«‹å¼€å‘ï¼‰

**ğŸ’¡ å…³é”®ç‰¹å¾æ ‡æ³¨**ï¼ˆå®¢è§‚äº‹å®ï¼Œä¸è¯„ä»·ï¼‰ï¼š
- æ¶‰åŠå¤–éƒ¨æ¥å£ï¼š[æ˜¯/å¦ï¼Œå“ªäº›]
- æ¶‰åŠæ”¯ä»˜æµç¨‹ï¼š[æ˜¯/å¦]
- æ¶‰åŠå®¡æ‰¹æµç¨‹ï¼š[æ˜¯/å¦ï¼Œå‡ çº§]
- æ¶‰åŠæ–‡ä»¶ä¸Šä¼ ï¼š[æ˜¯/å¦]

**âš ï¸ éœ€æ±‚é—®é¢˜**ï¼ˆå½±å“è¯„å®¡å†³ç­–ï¼‰ï¼š
- é€»è¾‘ä¸æ¸…æ™°ï¼š[å…·ä½“å“ªé‡Œ]
- é€»è¾‘çŸ›ç›¾ï¼š[å“ªé‡ŒçŸ›ç›¾]
- ç¼ºå¤±ä¿¡æ¯ï¼š[ç¼ºä»€ä¹ˆ]

**ğŸ¯ è¯„å®¡è®¨è®ºç‚¹**ï¼ˆä¾›ä¼šè®®è®¨è®ºï¼‰ï¼š
- ç»™äº§å“ï¼š[éœ€è¦æ¾„æ¸…çš„é—®é¢˜]
- ç»™å¼€å‘ï¼š[éœ€è¦æŠ€æœ¯è¯„ä¼°çš„ç‚¹]
- ç»™æµ‹è¯•ï¼š[æµ‹è¯•ç¯å¢ƒ/æ•°æ®å‡†å¤‡é—®é¢˜]
"""


def _get_stage4_prompt_developer() -> str:
    """è·å–å¼€å‘è§†è§’çš„ Stage 4 äº¤ä»˜ç‰© prompt"""
    return """
ã€STAGE 4 è¾“å‡ºè¦æ±‚ - å¼€å‘è§†è§’ã€‘

è¾“å‡ºç»“æ„ï¼š
1. # éœ€æ±‚æ–‡æ¡£æ€»ç»“
2. ## ğŸ“Š æ–‡æ¡£æ¦‚è§ˆï¼ˆé¡µé¢æ•°ã€æ¨¡å—æ•°ã€å˜æ›´ç±»å‹ç»Ÿè®¡ã€å¾…ç¡®è®¤é¡¹æ•°ï¼‰
3. ## ğŸ¯ éœ€æ±‚æ€§è´¨åˆ†æï¼ˆæ–°å¢/ä¿®æ”¹ç»Ÿè®¡è¡¨ + åˆ¤æ–­ä¾æ®ï¼‰
4. ## ğŸŒ å…¨å±€ä¸šåŠ¡æµç¨‹å›¾ï¼ˆâš¡æ ¸å¿ƒäº¤ä»˜ç‰©ï¼‰
   - åŒ…å«æ‰€æœ‰æ¨¡å—çš„å®Œæ•´ç»†èŠ‚
   - æ‰€æœ‰åˆ¤æ–­æ¡ä»¶ã€åˆ†æ”¯ã€å¼‚å¸¸å¤„ç†
   - æ‰€æœ‰å­—æ®µæ ¡éªŒè§„åˆ™å’Œæ•°æ®æµè½¬
   - æ¨¡å—é—´çš„è”ç³»å’Œæ•°æ®ä¼ é€’
   - ç”¨æ–‡å­—æµç¨‹å›¾ï¼ˆVertical Flow Diagramï¼‰
5. ## æ¨¡å—Xï¼šXXXæ¨¡å—
   ### åŠŸèƒ½æ¸…å•ï¼ˆè¡¨æ ¼ï¼‰
   ### å­—æ®µè§„åˆ™ï¼ˆè¡¨æ ¼ï¼‰
   ### æ¨¡å—æ€»ç»“ï¼ˆåˆ—ä¸¾å¼ï¼Œä¸ç”»å•ç‹¬æµç¨‹å›¾ï¼‰
6. ## âš ï¸ å¾…ç¡®è®¤äº‹é¡¹ï¼ˆæ‰€æœ‰ç–‘é—®æ±‡æ€»ï¼‰

è´¨é‡æ ‡å‡†ï¼šå¼€å‘çœ‹å®Œèƒ½å†™ä»£ç ï¼Œæµ‹è¯•çœ‹å®Œèƒ½å†™ç”¨ä¾‹ï¼Œ0é—æ¼
"""


def _get_stage4_prompt_tester() -> str:
    """è·å–æµ‹è¯•è§†è§’çš„ Stage 4 äº¤ä»˜ç‰© prompt"""
    return """
ã€STAGE 4 è¾“å‡ºè¦æ±‚ - æµ‹è¯•è§†è§’ã€‘

è¾“å‡ºç»“æ„ï¼š
1. # æµ‹è¯•è®¡åˆ’æ–‡æ¡£
2. ## ğŸ“Š æµ‹è¯•æ¦‚è§ˆ
   - æ¨¡å—æ•°ã€æµ‹è¯•åœºæ™¯æ•°ï¼ˆæ­£å‘Xä¸ªï¼Œå¼‚å¸¸Yä¸ªï¼‰
   - å˜æ›´ç±»å‹ç»Ÿè®¡ï¼ˆğŸ†•å…¨é‡æµ‹è¯• / ğŸ”„å›å½’æµ‹è¯•ï¼‰
3. ## ğŸ¯ éœ€æ±‚æ€§è´¨åˆ†æï¼ˆå½±å“æµ‹è¯•èŒƒå›´ï¼‰
4. ## æµ‹è¯•ç”¨ä¾‹æ¸…å•ï¼ˆæŒ‰æ¨¡å—ï¼‰
   ### æ¨¡å—Xï¼šXXX
   #### æ­£å‘åœºæ™¯ï¼ˆP0ï¼‰
   - åœºæ™¯1ï¼šå‰ç½®æ¡ä»¶ â†’ æ­¥éª¤ â†’ æœŸæœ›ç»“æœ
   - åœºæ™¯2ï¼š...
   #### å¼‚å¸¸åœºæ™¯ï¼ˆP1ï¼‰
   - å¼‚å¸¸1ï¼šè§¦å‘æ¡ä»¶ â†’ æœŸæœ›ç»“æœ
   #### å­—æ®µæ ¡éªŒè¡¨
   | å­—æ®µ | å¿…å¡« | è§„åˆ™ | é”™è¯¯æç¤º | è¾¹ç•Œå€¼æµ‹è¯• |
5. ## ğŸ“‹ æµ‹è¯•æ•°æ®å‡†å¤‡æ¸…å•
6. ## ğŸ”„ å›å½’æµ‹è¯•æç¤ºï¼ˆå¦‚æœ‰ä¿®æ”¹ç±»å‹æ¨¡å—ï¼‰
7. ## â“ æµ‹è¯•ç–‘é—®æ±‡æ€»ï¼ˆéœ€æ¾„æ¸…æ‰èƒ½å†™ç”¨ä¾‹ï¼‰

è´¨é‡æ ‡å‡†ï¼šæµ‹è¯•äººå‘˜æ‹¿åˆ°åå¯ç›´æ¥å†™ç”¨ä¾‹ï¼ŒçŸ¥é“æµ‹ä»€ä¹ˆã€æ€ä¹ˆæµ‹
"""


def _get_stage4_prompt_explorer() -> str:
    """è·å–å¿«é€Ÿæ¢ç´¢è§†è§’çš„ Stage 4 äº¤ä»˜ç‰© prompt"""
    return """
ã€STAGE 4 è¾“å‡ºè¦æ±‚ - å¿«é€Ÿæ¢ç´¢/éœ€æ±‚è¯„å®¡è§†è§’ã€‘

è¾“å‡ºç»“æ„ï¼ˆåƒè¯„å®¡ä¼šPPTï¼‰ï¼š
1. # éœ€æ±‚è¯„å®¡ - XXXåŠŸèƒ½
2. ## ğŸ“Š æ–‡æ¡£æ¦‚è§ˆï¼ˆ1åˆ†é’Ÿäº†è§£å…¨å±€ï¼‰
   - æ€»é¡µé¢æ•°ã€æ¨¡å—æ•°
   - éœ€æ±‚æ€§è´¨ç»Ÿè®¡ï¼ˆæ–°å¢Xä¸ª/ä¿®æ”¹Yä¸ªï¼‰
3. ## ğŸ¯ éœ€æ±‚æ€§è´¨åˆ†æ
   | å˜æ›´ç±»å‹ | æ¨¡å—æ•° | æ¨¡å—åˆ—è¡¨ | åˆ¤æ–­ä¾æ® |
4. ## ğŸ“¦ æ¨¡å—æ¸…å•è¡¨
   | åºå· | æ¨¡å—å | å˜æ›´ç±»å‹ | æ ¸å¿ƒåŠŸèƒ½ç‚¹(3-5ä¸ª) | ä¾èµ–æ¨¡å— | é¡µé¢æ•° |
5. ## ğŸ”„ æ•°æ®æµå‘å›¾ï¼ˆæ–‡å­—æˆ–ASCIIå›¾ï¼‰
   - å±•ç¤ºæ¨¡å—é—´ä¾èµ–å…³ç³»
   - æ•°æ®ä¼ é€’æ–¹å‘
6. ## ğŸ“… å¼€å‘é¡ºåºå»ºè®®ï¼ˆåŸºäºä¾èµ–å…³ç³»ï¼‰
   - ç¬¬ä¸€æ‰¹ï¼ˆæ— ä¾èµ–ï¼‰ï¼š...
   - ç¬¬äºŒæ‰¹ï¼ˆä¾èµ–ç¬¬ä¸€æ‰¹ï¼‰ï¼š...
   - å¯å¹¶è¡Œï¼š...
7. ## ğŸ”— å…³é”®ä¾èµ–å…³ç³»è¯´æ˜
   | æ¨¡å— | ä¾èµ–ä»€ä¹ˆ | ä¾èµ–åŸå›  | å½±å“ |
8. ## âš ï¸ é£é™©å’Œå¾…ç¡®è®¤äº‹é¡¹
   - éœ€æ±‚ä¸æ¸…æ™°ï¼š...
   - é€»è¾‘çŸ›ç›¾ï¼š...
   - å¤–éƒ¨ä¾èµ–ï¼š...
9. ## ğŸ’¼ å‰åç«¯åˆ†å·¥å‚è€ƒï¼ˆä»…ç½—åˆ—ï¼Œä¸ä¼°å·¥æ—¶ï¼‰
10. ## ğŸ“‹ è¯„å®¡ä¼šè®¨è®ºè¦ç‚¹
    - ç»™äº§å“ï¼š...
    - ç»™å¼€å‘ï¼š...
    - ç»™æµ‹è¯•ï¼š...
11. ## âœ… è¯„å®¡åè¡ŒåŠ¨é¡¹

ç¦æ­¢ï¼šè¯„ä¼°å·¥æ—¶ã€è¯„ä¼°å¤æ‚åº¦ã€åšä¸»è§‚è¯„ä»·
åªåšï¼šé™ˆè¿°äº‹å®ã€å±•ç¤ºå…³ç³»ã€åˆ—å‡ºé—®é¢˜
"""


def _get_analysis_mode_prompt(analysis_mode: str) -> dict:
    """
    æ ¹æ®åˆ†ææ¨¡å¼è·å–å¯¹åº”çš„ prompt
    
    Args:
        analysis_mode: åˆ†ææ¨¡å¼ (developer/tester/explorer)
    
    Returns:
        åŒ…å« stage2_prompt å’Œ stage4_prompt çš„å­—å…¸
    """
    if analysis_mode == "tester":
        return {
            "mode_name": "æµ‹è¯•è§†è§’",
            "mode_desc": "æå–æµ‹è¯•åœºæ™¯ã€æ ¡éªŒè§„åˆ™ã€å¼‚å¸¸æ¸…å•",
            "stage2_prompt": _get_stage2_prompt_tester(),
            "stage4_prompt": _get_stage4_prompt_tester()
        }
    elif analysis_mode == "explorer":
        return {
            "mode_name": "å¿«é€Ÿæ¢ç´¢",
            "mode_desc": "æå–æ ¸å¿ƒåŠŸèƒ½ã€ä¾èµ–å…³ç³»ã€è¯„å®¡è¦ç‚¹",
            "stage2_prompt": _get_stage2_prompt_explorer(),
            "stage4_prompt": _get_stage4_prompt_explorer()
        }
    else:  # developer (default)
        return {
            "mode_name": "å¼€å‘è§†è§’",
            "mode_desc": "æå–æ‰€æœ‰ç»†èŠ‚ã€å­—æ®µè§„åˆ™ã€å®Œæ•´æµç¨‹",
            "stage2_prompt": _get_stage2_prompt_developer(),
            "stage4_prompt": _get_stage4_prompt_developer()
        }


@mcp.tool()
async def lanhu_get_ai_analyze_page_result(
        url: Annotated[str, "Lanhu URL with docId parameter (indicates PRD/prototype document). Example: https://lanhuapp.com/web/#/item/project/product?tid=xxx&pid=xxx&docId=xxx. If you have an invite link, use lanhu_resolve_invite_link first!"],
        page_names: Annotated[Union[str, List[str]], "Page name(s) to analyze. Use 'all' for all pages, single name like 'é€€æ¬¾æµç¨‹', or list like ['é€€æ¬¾æµç¨‹', 'ç”¨æˆ·ä¸­å¿ƒ']. Get exact names from lanhu_get_pages first!"],
        mode: Annotated[str, "Analysis mode: 'text_only' (fast global scan, text only for overview) or 'full' (detailed analysis with images+text). Default: 'full'"] = "full",
        analysis_mode: Annotated[str, "Analysis perspective (MUST be chosen by user after STAGE 1): 'developer' (detailed for coding), 'tester' (test scenarios/validation), 'explorer' (quick overview for review). Default: 'developer'"] = "developer",
        ctx: Context = None
) -> List[Union[str, Image]]:
    """
    [PRD/Requirement Document] Analyze Lanhu Axure prototype pages - GET VISUAL CONTENT
    
    USE THIS WHEN user says: éœ€æ±‚æ–‡æ¡£, éœ€æ±‚, PRD, äº§å“æ–‡æ¡£, åŸå‹, äº¤äº’ç¨¿, Axure, çœ‹çœ‹éœ€æ±‚, å¸®æˆ‘çœ‹éœ€æ±‚, åˆ†æéœ€æ±‚, éœ€æ±‚åˆ†æ
    DO NOT USE for: UIè®¾è®¡å›¾, è®¾è®¡ç¨¿, è§†è§‰è®¾è®¡, åˆ‡å›¾ (use lanhu_get_ai_analyze_design_result instead)
    
    FOUR-STAGE WORKFLOW (ZERO OMISSION):
    1. STAGE 1: Call with mode="text_only" and page_names="all" for global text scan
       - Purpose: Build god's view, understand structure, design grouping strategy
       - Output: Text only (fast)
       - âš ï¸ IMPORTANT: After STAGE 1, MUST ask user to choose analysis_mode!
    
    2. STAGE 2: Call with mode="full" for each group (output format varies by analysis_mode)
       - developer: Extract ALL details (fields, rules, flows) - for coding
       - tester: Extract test scenarios, validation points, field rules - for test cases
       - explorer: Extract core functions only (3-5 points) - for requirement review
    
    3. STAGE 3: Reverse validation (format varies by analysis_mode)
    
    4. STAGE 4: Generate deliverable (format varies by analysis_mode)
       - developer: Detailed requirement doc + global flowchart
       - tester: Test plan + test case list + field validation table
       - explorer: Review PPT-style doc + module table + dependency diagram
    
    Returns:
        - mode="text_only": Text content only (for fast global scan)
        - mode="full": Visual + text (format determined by analysis_mode)
    """
    extractor = LanhuExtractor()

    try:
        # è®°å½•åä½œè€…
        user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
        project_id = get_project_id_from_url(url)
        if project_id:
            store = MessageStore(project_id)
            store.record_collaborator(user_name, user_role)
        
        # è§£æURLè·å–æ–‡æ¡£ID
        params = extractor.parse_url(url)
        doc_id = params['doc_id']

        # è®¾ç½®è¾“å‡ºç›®å½•ï¼ˆå†…éƒ¨å®ç°ï¼Œè‡ªåŠ¨ç®¡ç†ï¼‰
        resource_dir = str(DATA_DIR / f"axure_extract_{doc_id[:8]}")
        output_dir = str(DATA_DIR / f"axure_extract_{doc_id[:8]}_screenshots")

        # ä¸‹è½½èµ„æºï¼ˆæ”¯æŒæ™ºèƒ½ç¼“å­˜ï¼‰
        download_result = await extractor.download_resources(url, resource_dir)

        # å¦‚æœæ˜¯æ–°ä¸‹è½½æˆ–æ›´æ–°ï¼Œä¿®å¤HTML
        if download_result['status'] in ['downloaded', 'updated']:
            fix_html_files(resource_dir)

        # è·å–é¡µé¢åˆ—è¡¨
        pages_info = await extractor.get_pages_list(url)
        all_pages = pages_info['pages']

        # å¤„ç†page_nameså‚æ•° - æ„å»ºnameåˆ°filenameçš„æ˜ å°„
        page_map = {p['name']: p['filename'].replace('.html', '') for p in all_pages}

        if isinstance(page_names, str):
            if page_names.lower() == 'all':
                target_pages = [p['filename'].replace('.html', '') for p in all_pages]
                target_page_names = [p['name'] for p in all_pages]
            else:
                # å¦‚æœæ˜¯é¡µé¢æ˜¾ç¤ºåï¼Œè½¬æ¢ä¸ºæ–‡ä»¶å
                if page_names in page_map:
                    target_pages = [page_map[page_names]]
                    target_page_names = [page_names]
                else:
                    # ç›´æ¥ä½œä¸ºæ–‡ä»¶åä½¿ç”¨
                    target_pages = [page_names]
                    target_page_names = [page_names]
        else:
            # åˆ—è¡¨å½¢å¼
            target_pages = []
            target_page_names = []
            for pn in page_names:
                if pn in page_map:
                    target_pages.append(page_map[pn])
                    target_page_names.append(pn)
                else:
                    target_pages.append(pn)
                    target_page_names.append(pn)

        # æˆªå›¾ï¼ˆä¸éœ€è¦è¿”å›base64äº†ï¼Œç›´æ¥ä¿å­˜æ–‡ä»¶ï¼‰
        # ä¼ å…¥version_idç”¨äºæ™ºèƒ½ç¼“å­˜
        version_id = download_result.get('version_id', '')
        results = await screenshot_page_internal(resource_dir, target_pages, output_dir, return_base64=False, version_id=version_id)

        # æ„å»ºå“åº”
        cached_count = sum(1 for r in results if r.get('from_cache'))
        summary = {
            'total_requested': len(target_pages),
            'successful': sum(1 for r in results if r['success']),
            'failed': sum(1 for r in results if not r['success']),
        }

        # æå–æˆåŠŸçš„ç»“æœ
        success_results = [r for r in results if r['success']]

        # æ„å»ºè¿”å›å†…å®¹åˆ—è¡¨ï¼ˆå›¾æ–‡ç©¿æ’ï¼‰
        content = []

        # Add summary header - ç®€åŒ–æ˜¾ç¤ºï¼Œåªå‘ŠçŸ¥æ˜¯å¦å‘½ä¸­ç¼“å­˜
        all_from_cache = cached_count == len(target_pages) and cached_count > 0
        cache_hint = "âš¡" if all_from_cache else "âœ“"

        # Build reverse mapping from filename to display name
        filename_to_display = {p['filename'].replace('.html', ''): p['name'] for p in all_pages}

        # æ ¹æ®modeå†³å®šè¾“å‡ºæ ¼å¼
        is_text_only = (mode == "text_only")
        mode_indicator = "ğŸ“ TEXT_ONLY MODE" if is_text_only else "ğŸ“¸ FULL MODE"
        
        header_text = f"{cache_hint} {mode_indicator} | Version: {download_result['version_id'][:8]}...\n"
        header_text += f"ğŸ“Š Total {summary['successful']}/{summary['total_requested']} pages\n\n"
        
        if is_text_only:
            # TEXT_ONLYæ¨¡å¼çš„æç¤ºï¼ˆSTAGE 1å…¨å±€æ‰«æï¼‰
            header_text += "=" * 60 + "\n"
            header_text += "ğŸ“ STAGE 1: GLOBAL TEXT SCAN (Building God's View)\n"
            header_text += "=" * 60 + "\n"
            header_text += "ğŸ¯ Your Mission:\n"
            header_text += "  1. Quickly read ALL page texts below\n"
            header_text += "  2. Identify document structure (modules, flows, entities)\n"
            header_text += "  3. Output structured analysis (MUST use Markdown table)\n"
            header_text += "  4. Design grouping strategy based on business logic\n"
            header_text += "  5. Create TODOs for STAGE 2 detailed analysis\n\n"
            header_text += "âš ï¸ Important:\n"
            header_text += "  â€¢ This is text-only mode for fast overview\n"
            header_text += "  â€¢ No visual outputs in this stage\n"
            header_text += "  â€¢ Focus on understanding structure, not extracting details\n"
            header_text += "  â€¢ Details will be extracted in STAGE 2 (with images)\n"
            header_text += "=" * 60 + "\n"
        else:
            # FULLæ¨¡å¼çš„æç¤ºï¼ˆSTAGE 2è¯¦ç»†åˆ†æï¼‰
            # è·å–åˆ†ææ¨¡å¼å¯¹åº”çš„ prompt
            mode_prompts = _get_analysis_mode_prompt(analysis_mode)
            
            header_text += "=" * 60 + "\n"
            header_text += f"ğŸ¤– STAGE 2 åˆ†ææ¨¡å¼ï¼šã€{mode_prompts['mode_name']}ã€‘\n"
            header_text += f"ğŸ“‹ {mode_prompts['mode_desc']}\n"
            header_text += "=" * 60 + "\n"
            header_text += "ğŸ“¸ ç†è§£åŸåˆ™ï¼šè§†è§‰è¾“å‡ºä¸ºä¸»ï¼Œæ–‡æœ¬ä¸ºè¾…\n"
            header_text += "  â€¢ è§†è§‰è¾“å‡ºåŒ…å«å®Œæ•´UIã€æµç¨‹å›¾ã€äº¤äº’ç»†èŠ‚\n"
            header_text += "  â€¢ æ–‡æœ¬æä¾›å…³é”®ä¿¡æ¯æå–ä½†å¯èƒ½ä¸å®Œæ•´\n"
            header_text += "  â€¢ å»ºè®®ï¼šå…ˆçœ‹å›¾ç†è§£æ•´ä½“ï¼Œå†ç”¨æ–‡æœ¬å¿«é€Ÿå®šä½å…³é”®ç‚¹\n\n"
            
            # æ·»åŠ å½“å‰åˆ†ææ¨¡å¼çš„ Stage 2 prompt
            header_text += "=" * 60 + "\n"
            header_text += f"ğŸ• äºŒç‹—å·¥ä½œæŒ‡å¼•ï¼ˆ{mode_prompts['mode_name']}ï¼‰\n"
            header_text += "=" * 60 + "\n"
            header_text += "åˆ†æå®Œæœ¬ç»„é¡µé¢åï¼Œå¿…é¡»æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n"
            header_text += mode_prompts['stage2_prompt']
            header_text += "\n" + "=" * 60 + "\n"
            
            # æ·»åŠ  Stage 4 è¾“å‡ºæç¤ºï¼ˆä¾› AI è®°ä½ï¼‰
            header_text += "\nğŸ“ æé†’ï¼šSTAGE 4 äº¤ä»˜ç‰©æ ¼å¼ï¼ˆå®Œæˆæ‰€æœ‰åˆ†ç»„åä½¿ç”¨ï¼‰ï¼š\n"
            header_text += mode_prompts['stage4_prompt']
            header_text += "\n" + "=" * 60 + "\n\n"
        header_text += "ğŸ“‹ Return Format (due to MCP limitations):\n"
        header_text += "  1ï¸âƒ£ [ABOVE] All visual outputs displayed in page order (top to bottom)\n"
        header_text += "  2ï¸âƒ£ [BELOW] Corresponding document text content (top to bottom)\n\n"
        header_text += "ğŸ“Œ Image-Text Mapping:\n"
        if success_results:
            display_name = filename_to_display.get(success_results[0]['page_name'], success_results[0]['page_name'])
            header_text += f"  â€¢ Image 1 â†” Page 1 text: {display_name}\n"
        if len(success_results) > 1:
            display_name = filename_to_display.get(success_results[1]['page_name'], success_results[1]['page_name'])
            header_text += f"  â€¢ Image 2 â†” Page 2 text: {display_name}\n"
        if len(success_results) > 2:
            display_name = filename_to_display.get(success_results[2]['page_name'], success_results[2]['page_name'])
            header_text += f"  â€¢ Image 3 â†” Page 3 text: {display_name}\n"
        if len(success_results) > 3:
            display_name = filename_to_display.get(success_results[3]['page_name'], success_results[3]['page_name'])
            header_text += f"  â€¢ Image 4 â†” Page 4 text: {display_name}\n"
        if len(success_results) > 4:
            header_text += f"  â€¢ ... Total {len(success_results)} pages, and so on\n"
        header_text += "\nğŸ’¡ Please match visual outputs above with text below to understand each page's requirements\n"
        header_text += "=" * 60 + "\n"
        
        # å¦‚æœæ˜¯é¦–æ¬¡æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ï¼ˆTEXT_ONLYæ¨¡å¼ï¼‰ï¼Œæ·»åŠ STAGE1çš„å·¥ä½œæŒ‡å¼•
        if isinstance(page_names, str) and page_names.lower() == 'all' and is_text_only:
            header_text += "\n" + "ğŸ• " + "=" * 58 + "\n"
            header_text += "äºŒç‹—å·¥ä½œæŒ‡å¼•ï¼ˆSTAGE 1å…¨å±€æ‰«æï¼‰\n"
            header_text += "=" * 60 + "\n"
            header_text += "ğŸ“‹ æœ¬é˜¶æ®µä»»åŠ¡ï¼ˆå»ºç«‹ä¸Šå¸è§†è§’ï¼‰ï¼š\n\n"
            header_text += "1ï¸âƒ£ å¿«é€Ÿé˜…è¯»æ‰€æœ‰é¡µé¢æ–‡æœ¬\n"
            header_text += "2ï¸âƒ£ è¾“å‡ºæ–‡æ¡£ç»“æ„è¡¨ï¼ˆæ¨¡å—ã€é¡µé¢ã€åŠŸèƒ½ï¼‰\n"
            header_text += "3ï¸âƒ£ è¯†åˆ«ä¸šåŠ¡å…³è”å…³ç³»\n"
            header_text += "4ï¸âƒ£ è®¾è®¡åˆç†åˆ†ç»„ç­–ç•¥ï¼ˆåŸºäºä¸šåŠ¡é€»è¾‘ï¼‰\n"
            header_text += "5ï¸âƒ£ âš¡ã€å¿…é¡»ã€‘è¯¢é—®è€æ¿é€‰æ‹©åˆ†ææ¨¡å¼\n"
            header_text += "6ï¸âƒ£ åå‘æ›´æ–°TODOsï¼ˆç»†åŒ–STAGE2åˆ†ç»„ä»»åŠ¡ï¼‰\n\n"
            header_text += "=" * 60 + "\n"
            header_text += "âš ï¸ ã€é‡è¦ã€‘å®Œæˆæ‰«æåå¿…é¡»è¯¢é—®è€æ¿é€‰æ‹©åˆ†ææ¨¡å¼ï¼š\n"
            header_text += "=" * 60 + "\n"
            # æ ¹æ®ç”¨æˆ·è§’è‰²ç”Ÿæˆæ¨èçš„åˆ†ææ¨¡å¼é€‰é¡¹
            user_name_local, user_role_local = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
            mode_options_local = _get_analysis_mode_options_by_role(user_role_local)
            
            header_text += "è€æ¿ï¼ŒäºŒç‹—å·²ç»å¿«é€Ÿæµè§ˆå®Œæ‰€æœ‰é¡µé¢äº†ï¼\n\n"
            header_text += "ğŸ“Š æˆ‘å‘ç°äº†ä»¥ä¸‹æ¨¡å—ï¼š\n"
            header_text += "[æ­¤å¤„è¾“å‡ºæ¨¡å—è¡¨æ ¼]\n\n"
            header_text += "è¯·é—®æ‚¨å¸Œæœ›äºŒç‹—ä»¥ä»€ä¹ˆè§’åº¦æ¥åˆ†æï¼Ÿ\n"
            header_text += mode_options_local + "\n"
            header_text += 'æ‚¨ä¹Ÿå¯ä»¥è‡ªå®šä¹‰éœ€æ±‚ï¼Œæ¯”å¦‚"ç®€å•çœ‹çœ‹"ã€"åªçœ‹æ•°æ®æµå‘"ç­‰ã€‚\n\n'
            header_text += "âš ï¸ è¯·å‘Šè¯‰äºŒç‹—æ‚¨çš„é€‰æ‹©ï¼ŒäºŒç‹—æ‰èƒ½ç»§ç»­å¹²æ´»ï¼\n"
            header_text += "=" * 60 + "\n"
        
        content.append(header_text)

        # æ ¹æ®modeå†³å®šæ˜¯å¦æ·»åŠ æˆªå›¾
        if not is_text_only:
            # FULLæ¨¡å¼ï¼šå…ˆæ·»åŠ æ‰€æœ‰æˆªå›¾
            for r in success_results:
                if 'screenshot_path' in r:
                    content.append(Image(path=r['screenshot_path']))

        # Add all text content (æ ¼å¼æ ¹æ®modeä¸åŒ)
        if is_text_only:
            # TEXT_ONLYæ¨¡å¼ï¼šæ–‡æœ¬æ˜¯ä¸»è¦å†…å®¹
            text_section = "\n" + "=" * 60 + "\n"
            text_section += "ğŸ“ ALL PAGE TEXTS (For Global Understanding)\n"
            text_section += "=" * 60 + "\n"
            text_section += "ğŸ’¡ Read these texts to understand document structure\n"
            text_section += "ğŸ’¡ Identify modules, flows, and business logic\n"
            text_section += "ğŸ’¡ Then design reasonable grouping strategy for STAGE 2\n"
            text_section += "=" * 60 + "\n"
        else:
            # FULLæ¨¡å¼ï¼šæ–‡æœ¬æ˜¯è¾…åŠ©å†…å®¹
            text_section = "\n" + "=" * 60 + "\n"
            text_section += "ğŸ“ Document Text Content (Supplementary, visual outputs above are primary)\n"
            text_section += "=" * 60 + "\n"
            text_section += "âš ï¸ Important: Text may be incomplete, for complex flowcharts/tables refer to visual outputs\n"
            text_section += "ğŸ’¡ Text Purpose: Quick keyword search, find specific info, understand text descriptions\n"
            text_section += "=" * 60 + "\n"
        content.append(text_section)

        for idx, r in enumerate(success_results, 1):
            display_name = filename_to_display.get(r['page_name'], r['page_name'])

            page_text = f"\n{'â”€' * 60}\n"
            page_text += f"ğŸ“„ Page {idx}: {display_name}\n"
            page_text += f"{'â”€' * 60}\n"

            if 'page_text' in r and r['page_text']:
                page_text += r['page_text'] + "\n"
            else:
                page_text += "âš ï¸ No text content extracted (please refer to corresponding visual output above)\n"

            content.append(page_text)

        # Show failed pages (if any)
        failed_pages = [r for r in results if not r['success']]
        if failed_pages:
            failure_text = f"\n{'=' * 50}\n"
            failure_text += f"âš ï¸ Failed {len(failed_pages)} pages:\n"
            for r in failed_pages:
                failure_text += f"  âœ— {r['page_name']}: {r.get('error', 'Unknown')}\n"
            content.append(failure_text)

        return content
    finally:
        await extractor.close()


async def _get_designs_internal(extractor: LanhuExtractor, url: str) -> dict:
    """å†…éƒ¨å‡½æ•°ï¼šè·å–è®¾è®¡å›¾åˆ—è¡¨"""
    # è§£æURLè·å–å‚æ•°
    params = extractor.parse_url(url)

    # æ„å»ºè·å–è®¾è®¡å›¾åˆ—è¡¨çš„API URL
    api_url = (
        f"https://lanhuapp.com/api/project/images"
        f"?project_id={params['project_id']}"
        f"&team_id={params['team_id']}"
        f"&dds_status=1&position=1&show_cb_src=1&comment=1"
    )

    # å‘é€è¯·æ±‚
    response = await extractor.client.get(api_url)
    response.raise_for_status()
    data = response.json()

    if data.get('code') != '00000':
        return {
            'status': 'error',
            'message': data.get('msg', 'Unknown error')
        }

    # æå–è®¾è®¡å›¾ä¿¡æ¯
    project_data = data.get('data', {})
    images = project_data.get('images', [])

    design_list = []
    for idx, img in enumerate(images, 1):
        design_list.append({
            'index': idx,
            'id': img.get('id'),
            'name': img.get('name'),
            'width': img.get('width'),
            'height': img.get('height'),
            'url': img.get('url'),
            'has_comment': img.get('has_comment', False),
            'update_time': img.get('update_time')
        })

    return {
        'status': 'success',
        'project_name': project_data.get('name'),
        'total_designs': len(design_list),
        'designs': design_list
    }


@mcp.tool()
async def lanhu_get_designs(
    url: Annotated[str, "Lanhu URL WITHOUT docId (indicates UI design project, not PRD). Example: https://lanhuapp.com/web/#/item/project/stage?tid=xxx&pid=xxx. Required params: tid, pid (NO docId)"],
    ctx: Context = None
) -> dict:
    """
    [UI Design] Get Lanhu UI design image list - CALL THIS FIRST before analyzing designs
    
    USE THIS WHEN user says: UIè®¾è®¡å›¾, è®¾è®¡å›¾, è®¾è®¡ç¨¿, è§†è§‰è®¾è®¡, UIç¨¿, çœ‹çœ‹è®¾è®¡, å¸®æˆ‘çœ‹è®¾è®¡å›¾, è®¾è®¡è¯„å®¡
    DO NOT USE for: éœ€æ±‚æ–‡æ¡£, PRD, åŸå‹, äº¤äº’ç¨¿, Axure (use lanhu_get_pages instead)
    DO NOT USE for: åˆ‡å›¾, å›¾æ ‡, ç´ æ (use lanhu_get_design_slices instead)
    
    Purpose: Get list of UI design images from designers. Must call this BEFORE lanhu_get_ai_analyze_design_result.
    
    Returns:
        Design image list and project metadata
    """
    extractor = LanhuExtractor()
    try:
        # è®°å½•åä½œè€…
        user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
        project_id = get_project_id_from_url(url)
        if project_id:
            store = MessageStore(project_id)
            store.record_collaborator(user_name, user_role)
        
        result = await _get_designs_internal(extractor, url)
        
        # Add AI suggestion when there are many designs (>8)
        if result['status'] == 'success':
            total_designs = result.get('total_designs', 0)
            if total_designs > 8:
                result['ai_suggestion'] = {
                    'notice': f'This project contains {total_designs} design images, which is quite a lot',
                    'recommendation': 'Suggest asking Boss in a pleasing tone: whether to download all designs or key ones first. Praise the design quality.',
                    'user_prompt_template': f'Boss, this project has {total_designs} UI design images - the designer really put in effort! Would you like to:\n1. Download all {total_designs} design images (comprehensive UI understanding, ErGou will get them all!)\n2. Download key designs first (Tell ErGou which ones~)\nAwaiting your orders!',
                    'language_note': 'Respond in Chinese when talking to user'
                }
        
        return result
    finally:
        await extractor.close()


@mcp.tool()
async def lanhu_get_ai_analyze_design_result(
        url: Annotated[str, "Lanhu URL WITHOUT docId (indicates UI design project). Example: https://lanhuapp.com/web/#/item/project/stage?tid=xxx&pid=xxx"],
        design_names: Annotated[Union[str, List[str]], "Design name(s) to analyze. Use 'all' for all designs, single name like 'é¦–é¡µè®¾è®¡', or list like ['é¦–é¡µè®¾è®¡', 'ä¸ªäººä¸­å¿ƒ']. Get exact names from lanhu_get_designs first!"],
        ctx: Context = None
) -> List[Union[str, Image]]:
    """
    [UI Design] Analyze Lanhu UI design images - GET VISUAL CONTENT
    
    USE THIS WHEN user says: UIè®¾è®¡å›¾, è®¾è®¡å›¾, è®¾è®¡ç¨¿, è§†è§‰è®¾è®¡, UIç¨¿, çœ‹çœ‹è®¾è®¡, å¸®æˆ‘çœ‹è®¾è®¡å›¾, è®¾è®¡è¯„å®¡
    DO NOT USE for: éœ€æ±‚æ–‡æ¡£, PRD, åŸå‹, äº¤äº’ç¨¿, Axure (use lanhu_get_ai_analyze_page_result instead)
    DO NOT USE for: åˆ‡å›¾, å›¾æ ‡, ç´ æ (use lanhu_get_design_slices instead)
    
    WORKFLOW: First call lanhu_get_designs to get design list, then call this to analyze specific designs.
    
    Returns:
        Visual representation of UI design images
    """
    extractor = LanhuExtractor()
    try:
        # è®°å½•åä½œè€…
        user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
        project_id = get_project_id_from_url(url)
        if project_id:
            store = MessageStore(project_id)
            store.record_collaborator(user_name, user_role)
        
        # è§£æURLè·å–å‚æ•°
        params = extractor.parse_url(url)

        # è·å–è®¾è®¡å›¾åˆ—è¡¨
        designs_data = await _get_designs_internal(extractor, url)

        if designs_data['status'] != 'success':
            return [f"âŒ Failed to get design list: {designs_data.get('message', 'Unknown error')}"]

        designs = designs_data['designs']

        # ç¡®å®šè¦æˆªå›¾çš„è®¾è®¡å›¾
        if isinstance(design_names, str) and design_names.lower() == 'all':
            target_designs = designs
        else:
            if isinstance(design_names, str):
                design_names = [design_names]
            target_designs = [d for d in designs if d['name'] in design_names]

        if not target_designs:
            available_names = [d['name'] for d in designs]
            return [
                f"âš ï¸ No matching design found\n\nAvailable designs:\n" + "\n".join(f"  â€¢ {name}" for name in available_names)]

        # è®¾ç½®è¾“å‡ºç›®å½•ï¼ˆå†…éƒ¨å®ç°ï¼Œè‡ªåŠ¨ç®¡ç†ï¼‰
        output_dir = DATA_DIR / 'lanhu_designs' / params['project_id']
        output_dir.mkdir(parents=True, exist_ok=True)

        # ä¸‹è½½è®¾è®¡å›¾
        results = []
        for design in target_designs:
            try:
                # è·å–åŸå›¾URLï¼ˆå»æ‰OSSå¤„ç†å‚æ•°ï¼‰
                img_url = design['url'].split('?')[0]

                # ä¸‹è½½å›¾ç‰‡
                response = await extractor.client.get(img_url)
                response.raise_for_status()

                # ä¿å­˜æ–‡ä»¶
                filename = f"{design['name']}.png"
                filepath = output_dir / filename

                with open(filepath, 'wb') as f:
                    f.write(response.content)

                results.append({
                    'success': True,
                    'design_name': design['name'],
                    'screenshot_path': str(filepath)
                })
            except Exception as e:
                results.append({
                    'success': False,
                    'design_name': design['name'],
                    'error': str(e)
                })

        # Build return content
        content = []

        # Add summary text
        summary_text = f"ğŸ“Š Design Download\n"
        summary_text += f"ğŸ“ Project: {designs_data['project_name']}\n"
        summary_text += f"âœ“ {len([r for r in results if r['success']])}/{len(results)} designs\n\n"

        # Show design list
        summary_text += "ğŸ“‹ Design List (display order from top to bottom):\n"
        success_results = [r for r in results if r['success']]
        for idx, r in enumerate(success_results, 1):
            summary_text += f"{idx}. {r['design_name']}\n"

        # Show failed designs
        failed_results = [r for r in results if not r['success']]
        if failed_results:
            summary_text += f"\nâš ï¸ Failed {len(failed_results)} designs:\n"
            for r in failed_results:
                summary_text += f"  âœ— {r['design_name']}: {r.get('error', 'Unknown')}\n"

        content.append(summary_text)

        # æ·»åŠ æˆåŠŸçš„æˆªå›¾
        for r in results:
            if r['success'] and 'screenshot_path' in r:
                content.append(Image(path=r['screenshot_path']))

        return content
    finally:
        await extractor.close()


@mcp.tool()
async def lanhu_get_design_slices(
        url: Annotated[str, "Lanhu URL WITHOUT docId (indicates UI design project). Example: https://lanhuapp.com/web/#/item/project/stage?tid=xxx&pid=xxx"],
        design_name: Annotated[str, "Exact design name (single design only, NOT 'all'). Example: 'é¦–é¡µè®¾è®¡', 'ç™»å½•é¡µ'. Must match exactly with name from lanhu_get_designs result!"],
        include_metadata: Annotated[bool, "Include color, opacity, shadow info"] = True,
        ctx: Context = None
) -> dict:
    """
    [UI Slices/Assets] Get slice/asset info from Lanhu design for download
    
    USE THIS WHEN user says: åˆ‡å›¾, ä¸‹è½½åˆ‡å›¾, å›¾æ ‡, icon, ç´ æ, èµ„æº, å¯¼å‡ºåˆ‡å›¾, ä¸‹è½½ç´ æ, è·å–å›¾æ ‡
    DO NOT USE for: éœ€æ±‚æ–‡æ¡£, PRD, åŸå‹ (use lanhu_get_pages instead)
    DO NOT USE for: çœ‹è®¾è®¡å›¾, è®¾è®¡è¯„å®¡ (use lanhu_get_designs instead)
    
    WORKFLOW: First call lanhu_get_designs to get design list, then call this to get slices from specific design.
    
    Returns:
        Slice list with download URLs, AI will handle smart naming and batch download
    """
    extractor = LanhuExtractor()
    try:
        # è®°å½•åä½œè€…
        user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
        project_id = get_project_id_from_url(url)
        if project_id:
            store = MessageStore(project_id)
            store.record_collaborator(user_name, user_role)
        
        # 1. è·å–è®¾è®¡å›¾åˆ—è¡¨
        designs_data = await _get_designs_internal(extractor, url)

        if designs_data['status'] != 'success':
            return {
                'status': 'error',
                'message': designs_data.get('message', 'Failed to get designs')
            }

        # 2. æŸ¥æ‰¾æŒ‡å®šçš„è®¾è®¡å›¾
        target_design = None
        for design in designs_data['designs']:
            if design['name'] == design_name:
                target_design = design
                break

        if not target_design:
            available_names = [d['name'] for d in designs_data['designs']]
            return {
                'status': 'error',
                'message': f"Design '{design_name}' does not exist",
                'available_designs': available_names
            }

        # 3. è§£æURLè·å–å‚æ•°
        params = extractor.parse_url(url)

        # 4. è·å–åˆ‡å›¾ä¿¡æ¯
        slices_data = await extractor.get_design_slices_info(
            image_id=target_design['id'],
            team_id=params['team_id'],
            project_id=params['project_id'],
            include_metadata=include_metadata
        )

        # 5. Add AI workflow guide
        ai_workflow_guide = {
            "instructions": "ğŸ¤– AI assistant must follow this workflow to process slice download tasks",
            "language_requirement": "âš ï¸ IMPORTANT: Always respond to user in Chinese (ä¸­æ–‡å›å¤)",
            "workflow_steps": [
                {
                    "step": 1,
                    "title": "Create TODO Task Plan",
                    "tasks": [
                        "Analyze project structure (read package.json, pom.xml, requirements.txt, etc.)",
                        "Identify project type (React/Vue/Flutter/iOS/Android/Plain Frontend, etc.)",
                        "Determine slice storage directory (e.g., src/assets/images/)",
                        "Plan slice grouping strategy (by feature module, UI component, etc.)"
                    ]
                },
                {
                    "step": 2,
                    "title": "Smart Directory Selection Rules",
                    "rules": [
                        "Priority 1: If user explicitly specified output_dir â†’ use user-specified path",
                        "Priority 2: If project has standard assets directory â†’ use project convention (e.g., src/assets/images/slices/)",
                        "Priority 3: If generic project â†’ use design_slices/{design_name}/"
                    ],
                    "common_project_structures": {
                        "React/Vue": ["src/assets/", "public/images/"],
                        "Flutter": ["assets/images/"],
                        "iOS": ["Assets.xcassets/"],
                        "Android": ["res/drawable/", "res/mipmap/"],
                        "Plain Frontend": ["images/", "assets/"]
                    }
                },
                {
                    "step": 3,
                    "title": "Smart Naming Strategy",
                    "description": "Generate semantic filenames based on layer_path, parent_name, size",
                    "examples": [
                        {
                            "layer_path": "TopStatusBar/Battery/Border",
                            "size": "26x14",
                            "suggested_name": "status_bar_battery_border_26x14.png"
                        },
                        {
                            "layer_path": "Button/Background",
                            "size": "200x50",
                            "suggested_name": "button_background_200x50.png"
                        }
                    ],
                    "naming_patterns": {
                        "icons": "icon_xxx.png",
                        "backgrounds": "bg_xxx.png",
                        "buttons": "btn_xxx.png"
                    }
                },
                {
                    "step": 4,
                    "title": "Environment Detection and Download Solution Selection",
                    "principle": "AI must first detect current system environment and available tools, then autonomously select the best download solution",
                    "priority_rules": [
                        "Priority 1: Use system built-in download tools (curl/PowerShell/wget, etc.)",
                        "Priority 2: If system tools unavailable, detect programming language environment (python/node, etc.)",
                        "Priority 3: Create temporary script as last resort"
                    ],
                    "detection_steps": [
                        "Step 1: Detect operating system type (Windows/macOS/Linux)",
                        "Step 2: Sequentially detect available download tools",
                        "Step 3: Autonomously select optimal solution based on detection results",
                        "Step 4: Execute download task",
                        "Step 5: Clean up temporary files (if any)"
                    ],
                    "common_tools_by_platform": {
                        "Windows": {
                            "built_in": ["PowerShell Invoke-WebRequest", "certutil"],
                            "optional": ["curl (Win10 1803+ built-in)", "python", "node"]
                        },
                        "macOS": {
                            "built_in": ["curl"],
                            "optional": ["python", "wget", "node"]
                        },
                        "Linux": {
                            "built_in": ["curl", "wget"],
                            "optional": ["python", "node"]
                        }
                    },
                    "important_principles": [
                        "âš ï¸ Do not assume any tool is available, must detect first",
                        "âš ï¸ Prefer system built-in tools, avoid third-party dependencies",
                        "âš ï¸ Do not use fixed code templates or example code",
                        "âš ï¸ Dynamically generate commands or scripts based on actual environment",
                        "âš ï¸ Control concurrency when batch downloading",
                        "âš ï¸ Must clean up temporary files after completion"
                    ]
                }
            ],
            "execution_workflow": {
                "description": "Complete workflow that AI must autonomously complete",
                "steps": [
                    "Step 1: Call lanhu_get_design_slices(url, design_name) to get slice info",
                    "Step 2: Create TODO task plan (use todo_write tool)",
                    "Step 3: Detect current operating system type",
                    "Step 4: Detect available download tools by priority",
                    "Step 5: Identify project type and determine output directory",
                    "Step 6: Generate smart filenames based on slice info",
                    "Step 7: Select optimal download solution based on detection results",
                    "Step 8: Execute batch download task",
                    "Step 9: Verify download results",
                    "Step 10: Clean up temporary files and complete TODO"
                ]
            },
            "important_notes": [
                "ğŸ¯ AI must proactively complete the entire workflow, don't just return info and wait for user action",
                "ğŸ“‹ AI must use todo_write tool to create task plan, ensure orderly progress",
                "ğŸ” AI must detect environment and tool availability first, then select download solution",
                "â­ AI must prefer system built-in tools, avoid third-party dependencies",
                "ğŸš« AI must not use fixed code examples, must dynamically generate commands based on actual environment",
                "âœ¨ AI must smartly select output directory based on project structure, don't blindly use default path",
                "ğŸ·ï¸ AI must generate semantic filenames based on slice's layer_path and parent_name",
                "ğŸ’» AI must select corresponding download tools for different OS (Windows/macOS/Linux)",
                "ğŸ§¹ AI must clean up temporary files after completion (if any)",
                "ğŸ—£ï¸ AI must always respond to user in Chinese (ä¸­æ–‡å›å¤)"
            ]
        }

        return {
            'status': 'success',
            **slices_data,
            'ai_workflow_guide': ai_workflow_guide
        }

    except Exception as e:
        return {
            'status': 'error',
            'message': str(e)
        }
    finally:
        await extractor.close()


# ==================== å›¢é˜Ÿç•™è¨€æ¿åŠŸèƒ½ ====================

@mcp.tool()
async def lanhu_say(
        url: Annotated[str, "è“æ¹–URLï¼ˆå«tidå’Œpidï¼‰ã€‚ä¾‹: https://lanhuapp.com/web/#/item/project/product?tid=xxx&pid=xxx&docId=xxxã€‚ä¼šè‡ªåŠ¨æå–é¡¹ç›®å’Œæ–‡æ¡£ä¿¡æ¯"],
        summary: Annotated[str, "ç•™è¨€æ ‡é¢˜/æ¦‚è¦"],
        content: Annotated[str, "ç•™è¨€è¯¦ç»†å†…å®¹"],
        mentions: Annotated[Optional[List[str]], "âš ï¸@æé†’äººåã€‚å¿…é¡»æ˜¯å…·ä½“äººå: äº‘é¹¤/å°åº/è èœ/èŒœèŒœ/å°å‡¡/ç¦ç‘/ç›Šè¾¾/æ›¼åŸ/å‡‰ç³•/é›¨ç§‹/ä¸ƒé›¶/ç«¥æ¸Š/æ˜“æ°´/è‹¥å…°/ç‰§ä¹‹/æ›¼è‰/å°æ™´/æµ·é£ã€‚ç¦æ­¢ä½¿ç”¨è§’è‰²å(åç«¯/å‰ç«¯ç­‰)ï¼"] = None,
        message_type: Annotated[Optional[str], "ç•™è¨€ç±»å‹ã€‚å¯é€‰: normal(æ™®é€šç•™è¨€), task(æŸ¥è¯¢ä»»åŠ¡-ä»…é™æŸ¥è¯¢æ“ä½œ,ç¦æ­¢ä¿®æ”¹ä»£ç ), question(éœ€è¦å›ç­”çš„é—®é¢˜), urgent(ç´§æ€¥é€šçŸ¥), knowledge(çŸ¥è¯†åº“-é•¿æœŸä¿å­˜çš„ç»éªŒçŸ¥è¯†)ã€‚é»˜è®¤: normal"] = None,
        ctx: Context = None
) -> dict:
    """
    Post message to team message board
    
    USE THIS WHEN user says: æœ‰è¯è¯´, ç•™è¨€, å‘æ¶ˆæ¯, é€šçŸ¥å›¢é˜Ÿ, å‘Šè¯‰xxx, @äº‘é¹¤, @å°åº, å…±äº«ç»™xxx, åˆ†äº«ç»™xxx, å‘ç»™xxx, å†™ç»™xxx, è½¬å‘ç»™xxx
    
    Message type description:
    - normal: Normal message/notification (default)
    - task: Query task - Only for query operations (query code, query database, query TODO, etc.), NO code modification
    - question: Question message - Needs answer from others
    - urgent: Urgent message - Needs immediate attention
    - knowledge: Knowledge base - Long-term preserved experience, pitfalls, notes, best practices
    
    Security restrictions:
    task type can only be used for query operations, including:
    - Query code location, code logic
    - Query database table structure, data
    - Query test methods, test coverage
    - Query TODO, comments
    - Forbidden: Modify code, delete files, execute commands, commit code
    
    Knowledge use cases:
    - Pitfalls encountered and solutions
    - Testing notes
    - Development experience and best practices
    - Common FAQ
    - Technical decision records
    
    Purpose: Post message to project message board, can @ specific person to send Feishu notification
    
    Returns:
        Post result, including message ID and details
    """
    # è·å–ç”¨æˆ·ä¿¡æ¯
    user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
    
    # è·å–project_id
    project_id = get_project_id_from_url(url)
    if not project_id:
        return {"status": "error", "message": "æ— æ³•ä»URLè§£æproject_id"}
    
    # è·å–å…ƒæ•°æ®ï¼ˆè‡ªåŠ¨ï¼Œå¸¦ç¼“å­˜ï¼‰
    metadata = await _fetch_metadata_from_url(url)
    
    # éªŒè¯message_type
    valid_types = ['normal', 'task', 'question', 'urgent', 'knowledge']
    if message_type and message_type not in valid_types:
        return {
            "status": "error",
            "message": f"æ— æ•ˆçš„ç•™è¨€ç±»å‹: {message_type}",
            "valid_types": valid_types
        }
    
    # é»˜è®¤ä¸ºnormal
    if not message_type:
        message_type = 'normal'
    
    # éªŒè¯mentionsï¼ˆåªèƒ½@å…·ä½“äººåï¼‰
    if mentions:
        invalid_names = [name for name in mentions if name not in MENTION_ROLES]
        if invalid_names:
            return {
                "status": "error", 
                "message": f"æ— æ•ˆçš„äººå: {invalid_names}ã€‚åªèƒ½@å…·ä½“äººåï¼Œä¸èƒ½ä½¿ç”¨è§’è‰²åï¼",
                "valid_names": MENTION_ROLES
            }
    
    # ä¿å­˜æ¶ˆæ¯
    store = MessageStore(project_id)
    store.record_collaborator(user_name, user_role)
    
    # ä¿å­˜é¡¹ç›®å…ƒæ•°æ®åˆ°storeï¼ˆå¦‚æœé¦–æ¬¡è·å–åˆ°ï¼‰
    if metadata.get('project_name') and not store._data.get('project_name'):
        store._data['project_name'] = metadata['project_name']
    if metadata.get('folder_name') and not store._data.get('folder_name'):
        store._data['folder_name'] = metadata['folder_name']
    store._save()
    
    message = store.save_message(
        summary=summary,
        content=content,
        author_name=user_name,
        author_role=user_role,
        mentions=mentions or [],
        message_type=message_type,  # æ–°å¢ï¼šç•™è¨€ç±»å‹
        # æ ‡å‡†å…ƒæ•°æ®ï¼ˆ10ä¸ªå­—æ®µï¼‰
        project_name=metadata.get('project_name'),
        folder_name=metadata.get('folder_name'),
        doc_id=metadata.get('doc_id'),
        doc_name=metadata.get('doc_name'),
        doc_type=metadata.get('doc_type'),
        doc_version=metadata.get('doc_version'),
        doc_updated_at=metadata.get('doc_updated_at'),
        doc_url=metadata.get('doc_url')
    )
    
    # å‘é€é£ä¹¦é€šçŸ¥ï¼ˆæ— è®ºæ˜¯å¦@äººéƒ½å‘é€ï¼‰
    try:
        await send_feishu_notification(
            summary=summary,
            content=content,
            author_name=user_name,
            author_role=user_role,
            mentions=mentions or [],
            message_type=message_type,
            project_name=metadata.get('project_name'),
            doc_name=metadata.get('doc_name'),
            doc_url=metadata.get('doc_url')
        )
    except Exception as e:
        # é£ä¹¦é€šçŸ¥å¤±è´¥ä¸å½±å“ç•™è¨€å‘å¸ƒ
        print(f"âš ï¸ é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥ï¼ˆä¸å½±å“ç•™è¨€å‘å¸ƒï¼‰: {e}")
    
    return {
        "status": "success",
        "message": "ç•™è¨€å‘å¸ƒæˆåŠŸ",
        "data": {
            "id": message["id"],
            "summary": message["summary"],
            "message_type": message["message_type"],  # æ–°å¢ï¼šç•™è¨€ç±»å‹
            "mentions": message["mentions"],
            "author_name": message["author_name"],
            "author_role": message["author_role"],
            "created_at": message["created_at"],
            # å®Œæ•´çš„10ä¸ªå…ƒæ•°æ®å­—æ®µ
            "project_id": project_id,
            "project_name": message.get("project_name"),
            "folder_name": message.get("folder_name"),
            "doc_id": message.get("doc_id"),
            "doc_name": message.get("doc_name"),
            "doc_type": message.get("doc_type"),
            "doc_version": message.get("doc_version"),
            "doc_updated_at": message.get("doc_updated_at"),
            "doc_url": message.get("doc_url")
        }
    }


@mcp.tool()
async def lanhu_say_list(
    url: Annotated[Optional[str], "è“æ¹–URLæˆ–'all'ã€‚ä¸ä¼ æˆ–ä¼ 'all'=æŸ¥è¯¢æ‰€æœ‰é¡¹ç›®ï¼›ä¼ å…·ä½“URL=æŸ¥è¯¢å•ä¸ªé¡¹ç›®"] = None,
    filter_type: Annotated[Optional[str], "ç­›é€‰ç•™è¨€ç±»å‹: normal/task/question/urgent/knowledgeã€‚ä¸ä¼ åˆ™è¿”å›æ‰€æœ‰ç±»å‹"] = None,
    search_regex: Annotated[Optional[str], "æ­£åˆ™è¡¨è¾¾å¼æœç´¢ï¼ˆåœ¨summaryå’Œcontentä¸­åŒ¹é…ï¼‰ã€‚ä¾‹: 'æµ‹è¯•|é€€æ¬¾|å‘'ã€‚å»ºè®®ä½¿ç”¨ä»¥é¿å…è¿”å›è¿‡å¤šæ¶ˆæ¯"] = None,
    limit: Annotated[Optional[int], "é™åˆ¶è¿”å›æ¶ˆæ¯æ•°é‡ï¼ˆé˜²æ­¢ä¸Šä¸‹æ–‡çˆ†ç‚¸ï¼‰ã€‚ä¸ä¼ åˆ™ä¸é™åˆ¶"] = None,
    ctx: Context = None
) -> dict:
    """
    Get message list with filtering and search
    
    USE THIS WHEN user says: æŸ¥çœ‹ç•™è¨€, æœ‰ä»€ä¹ˆæ¶ˆæ¯, è°@æˆ‘äº†, ç•™è¨€åˆ—è¡¨, æ¶ˆæ¯åˆ—è¡¨
    
    Supports two modes:
    1. Provide specific URL: Query messages in that project
    2. url='all' or url=None: Query messages in all projects (global mode)
    
    Important: To prevent AI context overflow, it is recommended:
    1. Use filter_type to filter by type
    2. Use search_regex for further filtering (regex, AI can generate itself)
    3. Use limit to limit the number of returned messages
    4. Unless user explicitly requests "view all", filters must be used
    
    Example:
    - Query all knowledge: filter_type="knowledge"
    - Search containing "test" or "refund": search_regex="test|refund"
    - Query tasks and containing "database": filter_type="task", search_regex="database"
    - Limit to 10 latest: limit=10
    
    Purpose: Get message board message summary list, supports type filtering, regex search and quantity limit
    
    Returns:
        Message list, including mentions_me count
    """
    # è·å–ç”¨æˆ·ä¿¡æ¯
    user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
    
    # éªŒè¯filter_type
    if filter_type:
        valid_types = ['normal', 'task', 'question', 'urgent', 'knowledge']
        if filter_type not in valid_types:
            return {
                "status": "error",
                "message": f"æ— æ•ˆçš„ç±»å‹: {filter_type}",
                "valid_types": valid_types
            }
    
    # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå¦‚æœæä¾›ï¼‰
    import re
    regex_pattern = None
    if search_regex:
        try:
            regex_pattern = re.compile(search_regex, re.IGNORECASE)
        except re.error as e:
            return {
                "status": "error",
                "message": f"æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼: {search_regex}",
                "error": str(e)
            }
    
    # å…¨å±€æŸ¥è¯¢æ¨¡å¼
    if not url or url.lower() == 'all':
        store = MessageStore(project_id=None)
        groups = store.get_all_messages_grouped(user_role=user_role, user_name=user_name)
        
        # åº”ç”¨ç­›é€‰å’Œæœç´¢
        filtered_groups = []
        total_messages_before_filter = sum(g['message_count'] for g in groups)
        
        for group in groups:
            filtered_messages = []
            for msg in group['messages']:
                # ç±»å‹ç­›é€‰
                if filter_type and msg.get('message_type') != filter_type:
                    continue
                
                # æ­£åˆ™æœç´¢
                if regex_pattern:
                    text = f"{msg.get('summary', '')} {msg.get('content', '')}"
                    if not regex_pattern.search(text):
                        continue
                
                filtered_messages.append(msg)
            
            # å¦‚æœè¯¥ç»„æœ‰åŒ¹é…çš„æ¶ˆæ¯
            if filtered_messages:
                group_copy = group.copy()
                group_copy['messages'] = filtered_messages
                group_copy['message_count'] = len(filtered_messages)
                group_copy['mentions_me_count'] = sum(1 for m in filtered_messages if m.get('mentions_me'))
                filtered_groups.append(group_copy)
        
        # åº”ç”¨limitï¼ˆé™åˆ¶æ¶ˆæ¯æ€»æ•°ï¼‰
        if limit and limit > 0:
            limited_groups = []
            remaining_limit = limit
            for group in filtered_groups:
                if remaining_limit <= 0:
                    break
                group_copy = group.copy()
                group_copy['messages'] = group['messages'][:remaining_limit]
                group_copy['message_count'] = len(group_copy['messages'])
                limited_groups.append(group_copy)
                remaining_limit -= group_copy['message_count']
            filtered_groups = limited_groups
        
        # ç»Ÿè®¡
        total_messages = sum(g['message_count'] for g in filtered_groups)
        total_mentions_me = sum(g['mentions_me_count'] for g in filtered_groups)
        total_projects = len(set(g.get('project_id') for g in filtered_groups if g.get('project_id')))
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è­¦å‘Šï¼ˆæ— ç­›é€‰ä¸”æ¶ˆæ¯è¿‡å¤šï¼‰
        warning_message = None
        if not filter_type and not search_regex and not limit and total_messages_before_filter > 100:
            warning_message = f"âš ï¸ å‘ç°{total_messages_before_filter}æ¡ç•™è¨€ï¼Œå»ºè®®ä½¿ç”¨ç­›é€‰æ¡ä»¶é¿å…ä¸Šä¸‹æ–‡æº¢å‡ºã€‚ä½¿ç”¨ filter_type æˆ– search_regex æˆ– limit å‚æ•°"
        
        result = {
            "status": "success",
            "mode": "global",
            "current_user": {"name": user_name, "role": user_role},
            "total_messages": total_messages,
            "total_groups": len(filtered_groups),
            "total_projects": total_projects,
            "mentions_me_count": total_mentions_me,
            "groups": filtered_groups
        }
        
        if warning_message:
            result["warning"] = warning_message
        
        if filter_type or search_regex:
            result["filter_info"] = {
                "filter_type": filter_type,
                "search_regex": search_regex,
                "total_before_filter": total_messages_before_filter,
                "total_after_filter": total_messages
            }
        
        return result
    
    # å•é¡¹ç›®æŸ¥è¯¢æ¨¡å¼
    project_id = get_project_id_from_url(url)
    if not project_id:
        return {"status": "error", "message": "æ— æ³•ä»URLè§£æproject_id"}
    
    # è·å–æ¶ˆæ¯åˆ—è¡¨
    store = MessageStore(project_id)
    store.record_collaborator(user_name, user_role)
    messages = store.get_messages(user_role=user_role)
    
    # åº”ç”¨ç­›é€‰å’Œæœç´¢
    total_messages_before_filter = len(messages)
    filtered_messages = []
    
    for msg in messages:
        # ç±»å‹ç­›é€‰
        if filter_type and msg.get('message_type') != filter_type:
            continue
        
        # æ­£åˆ™æœç´¢
        if regex_pattern:
            text = f"{msg.get('summary', '')} {msg.get('content', '')}"
            if not regex_pattern.search(text):
                continue
        
        filtered_messages.append(msg)
    
    # åº”ç”¨limit
    if limit and limit > 0:
        filtered_messages = filtered_messages[:limit]
    
    # ç»Ÿè®¡@è‡ªå·±çš„æ¶ˆæ¯æ•°
    mentions_me_count = sum(1 for msg in filtered_messages if msg.get("mentions_me"))
    
    # æŒ‰æ–‡æ¡£åˆ†ç»„ï¼ˆå‡å°‘tokenï¼‰
    from collections import defaultdict
    groups_dict = defaultdict(list)
    
    for msg in filtered_messages:
        doc_id = msg.get('doc_id', 'no_doc')
        groups_dict[doc_id].append(msg)
    
    # æ„å»ºåˆ†ç»„ç»“æœ
    groups = []
    meta_fields = {
        'project_id', 'project_name', 'folder_name',
        'doc_id', 'doc_name', 'doc_type', 'doc_version',
        'doc_updated_at', 'doc_url'
    }
    
    for doc_id, doc_messages in groups_dict.items():
        if not doc_messages:
            continue
        
        # æå–å…ƒæ•°æ®ï¼ˆç»„å†…å…±äº«ï¼‰
        first_msg = doc_messages[0]
        
        group = {
            # å…ƒæ•°æ®ï¼ˆåªå‡ºç°ä¸€æ¬¡ï¼‰
            "doc_id": first_msg.get('doc_id'),
            "doc_name": first_msg.get('doc_name'),
            "doc_type": first_msg.get('doc_type'),
            "doc_version": first_msg.get('doc_version'),
            "doc_updated_at": first_msg.get('doc_updated_at'),
            "doc_url": first_msg.get('doc_url'),
            
            # ç»Ÿè®¡
            "message_count": len(doc_messages),
            "mentions_me_count": sum(1 for m in doc_messages if m.get("mentions_me")),
            
            # ç²¾ç®€æ¶ˆæ¯åˆ—è¡¨ï¼ˆç§»é™¤å…ƒæ•°æ®ï¼‰
            "messages": [_clean_message_dict({k: v for k, v in m.items() if k not in meta_fields}, user_name) for m in doc_messages]
        }
        
        groups.append(group)
    
    # æŒ‰ç»„å†…æœ€æ–°æ¶ˆæ¯æ—¶é—´æ’åº
    groups.sort(
        key=lambda g: max((m.get('created_at', '') for m in g['messages']), default=''),
        reverse=True
    )
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è­¦å‘Š
    warning_message = None
    if not filter_type and not search_regex and not limit and total_messages_before_filter > 50:
        warning_message = f"âš ï¸ è¯¥é¡¹ç›®æœ‰{total_messages_before_filter}æ¡ç•™è¨€ï¼Œå»ºè®®ä½¿ç”¨ç­›é€‰æ¡ä»¶é¿å…ä¸Šä¸‹æ–‡æº¢å‡º"
    
    result = {
        "status": "success",
        "mode": "single_project",
        "project_id": project_id,
        "project_name": store._data.get('project_name'),
        "folder_name": store._data.get('folder_name'),
        "current_user": {"name": user_name, "role": user_role},
        "total_messages": len(filtered_messages),
        "total_groups": len(groups),
        "mentions_me_count": mentions_me_count,
        "groups": groups
    }
    
    if warning_message:
        result["warning"] = warning_message
    
    if filter_type or search_regex:
        result["filter_info"] = {
            "filter_type": filter_type,
            "search_regex": search_regex,
            "total_before_filter": total_messages_before_filter,
            "total_after_filter": len(filtered_messages)
        }
    
    return result


@mcp.tool()
async def lanhu_say_detail(
        message_ids: Annotated[Union[int, List[int]], "æ¶ˆæ¯IDã€‚å•ä¸ªæ•°å­—æˆ–æ•°ç»„ã€‚ä¾‹: 1 æˆ– [1,2,3]"],
        url: Annotated[Optional[str], "è“æ¹–URLã€‚ä¼ URLåˆ™è‡ªåŠ¨è§£æé¡¹ç›®IDï¼›ä¸ä¼ åˆ™éœ€æ‰‹åŠ¨æä¾›project_idå‚æ•°"] = None,
        project_id: Annotated[Optional[str], "é¡¹ç›®IDã€‚ä»…åœ¨ä¸ä¼ urlæ—¶éœ€è¦ï¼Œç”¨äºå…¨å±€æŸ¥è¯¢æ¨¡å¼"] = None,
        ctx: Context = None
) -> dict:
    """
    Get message detail (supports batch query)
    
    USE THIS WHEN user says: æŸ¥çœ‹è¯¦æƒ…, çœ‹çœ‹å†…å®¹, è¯¦ç»†å†…å®¹, æ¶ˆæ¯è¯¦æƒ…
    
    Two modes:
    1. Provide url: Parse project_id from url, query messages in that project
    2. url='all'/None + project_id: Global mode, need to manually specify project_id
    
    Purpose: Get full content of messages by message ID
    
    Returns:
        Message detail list with full content
    """
    # è·å–ç”¨æˆ·ä¿¡æ¯
    user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
    
    # ç¡®å®šproject_id
    if url and url.lower() != 'all':
        target_project_id = get_project_id_from_url(url)
    elif project_id:
        target_project_id = project_id
    else:
        return {"status": "error", "message": "è¯·æä¾›urlæˆ–project_id"}
    
    if not target_project_id:
        return {"status": "error", "message": "æ— æ³•è·å–project_id"}
    
    # å¤„ç†message_idså‚æ•°
    if isinstance(message_ids, int):
        message_ids = [message_ids]
    
    # è·å–æ¶ˆæ¯è¯¦æƒ…
    store = MessageStore(target_project_id)
    store.record_collaborator(user_name, user_role)
    
    messages = []
    not_found = []
    
    for msg_id in message_ids:
        msg = store.get_message_by_id(msg_id, user_role=user_role)
        if msg:
            messages.append(msg)
        else:
            not_found.append(msg_id)
    
    return {
        "status": "success",
        "total": len(messages),
        "messages": messages,
        "not_found": not_found
    }


@mcp.tool()
async def lanhu_say_edit(
        url: Annotated[str, "è“æ¹–URLï¼ˆå«tidå’Œpidï¼‰"],
        message_id: Annotated[int, "è¦ç¼–è¾‘çš„æ¶ˆæ¯ID"],
        summary: Annotated[Optional[str], "æ–°æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™ä¸ä¿®æ”¹ï¼‰"] = None,
        content: Annotated[Optional[str], "æ–°å†…å®¹ï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™ä¸ä¿®æ”¹ï¼‰"] = None,
        mentions: Annotated[Optional[List[str]], "æ–°@åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™ä¸ä¿®æ”¹ï¼‰"] = None,
        ctx: Context = None
) -> dict:
    """
    Edit message
    
    USE THIS WHEN user says: ç¼–è¾‘ç•™è¨€, ä¿®æ”¹æ¶ˆæ¯, æ›´æ–°å†…å®¹
    
    Purpose: Edit published message, will record editor and edit time
    
    Returns:
        Updated message details
    """
    # è·å–ç”¨æˆ·ä¿¡æ¯
    user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
    
    # è·å–project_id
    project_id = get_project_id_from_url(url)
    if not project_id:
        return {"status": "error", "message": "æ— æ³•ä»URLè§£æproject_id"}
    
    # éªŒè¯mentionsï¼ˆåªèƒ½@å…·ä½“äººåï¼‰
    if mentions:
        invalid_names = [name for name in mentions if name not in MENTION_ROLES]
        if invalid_names:
            return {
                "status": "error", 
                "message": f"æ— æ•ˆçš„äººå: {invalid_names}ã€‚åªèƒ½@å…·ä½“äººåï¼Œä¸èƒ½ä½¿ç”¨è§’è‰²åï¼",
                "valid_names": MENTION_ROLES
            }
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ–°å†…å®¹
    if summary is None and content is None and mentions is None:
        return {"status": "error", "message": "è¯·è‡³å°‘æä¾›ä¸€ä¸ªè¦æ›´æ–°çš„å­—æ®µ"}
    
    # æ›´æ–°æ¶ˆæ¯
    store = MessageStore(project_id)
    store.record_collaborator(user_name, user_role)
    
    updated_msg = store.update_message(
        msg_id=message_id,
        editor_name=user_name,
        editor_role=user_role,
        summary=summary,
        content=content,
        mentions=mentions
    )
    
    if not updated_msg:
        return {"status": "error", "message": "æ¶ˆæ¯ä¸å­˜åœ¨", "message_id": message_id}
    
    # å‘é€é£ä¹¦ç¼–è¾‘é€šçŸ¥
    try:
        # è·å–å…ƒæ•°æ®
        metadata = await _fetch_metadata_from_url(url)
        
        await send_feishu_notification(
            summary=f"ğŸ”„ [å·²ç¼–è¾‘] {updated_msg.get('summary', '')}",
            content=updated_msg.get('content', ''),
            author_name=f"{user_name}(ç¼–è¾‘)",
            author_role=user_role,
            mentions=updated_msg.get('mentions', []),
            message_type=updated_msg.get('message_type', 'normal'),
            project_name=metadata.get('project_name'),
            doc_name=metadata.get('doc_name'),
            doc_url=metadata.get('doc_url')
        )
    except Exception as e:
        print(f"âš ï¸ é£ä¹¦ç¼–è¾‘é€šçŸ¥å‘é€å¤±è´¥ï¼ˆä¸å½±å“ç¼–è¾‘ï¼‰: {e}")
    
    return {
        "status": "success",
        "message": "æ¶ˆæ¯æ›´æ–°æˆåŠŸ",
        "data": updated_msg
    }


@mcp.tool()
async def lanhu_say_delete(
        url: Annotated[str, "è“æ¹–URLï¼ˆå«tidå’Œpidï¼‰"],
        message_id: Annotated[int, "è¦åˆ é™¤çš„æ¶ˆæ¯ID"],
        ctx: Context = None
) -> dict:
    """
    Delete message
    
    USE THIS WHEN user says: åˆ é™¤ç•™è¨€, åˆ é™¤æ¶ˆæ¯, ç§»é™¤
    
    Purpose: Delete published message
    
    Returns:
        Delete result
    """
    # è·å–ç”¨æˆ·ä¿¡æ¯
    user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
    
    # è·å–project_id
    project_id = get_project_id_from_url(url)
    if not project_id:
        return {"status": "error", "message": "æ— æ³•ä»URLè§£æproject_id"}
    
    # åˆ é™¤æ¶ˆæ¯
    store = MessageStore(project_id)
    store.record_collaborator(user_name, user_role)
    
    success = store.delete_message(message_id)
    
    if not success:
        return {"status": "error", "message": "æ¶ˆæ¯ä¸å­˜åœ¨", "message_id": message_id}
    
    return {
        "status": "success",
        "message": "æ¶ˆæ¯åˆ é™¤æˆåŠŸ",
        "deleted_id": message_id,
        "deleted_by_name": user_name,
        "deleted_by_role": user_role
    }


@mcp.tool()
async def lanhu_get_members(
    url: Annotated[str, "è“æ¹–URLï¼ˆå«tidå’Œpidï¼‰"],
    ctx: Context = None
) -> dict:
    """
    Get project collaborators list
    
    USE THIS WHEN user says: è°å‚ä¸äº†, åä½œè€…, å›¢é˜Ÿæˆå‘˜, æœ‰å“ªäº›äºº
    
    Purpose: Get list of team members who have used Lanhu MCP tools to access this project
    
    Returns:
        Collaborator list with first and last access time
    """
    # è·å–ç”¨æˆ·ä¿¡æ¯
    user_name, user_role = get_user_info(ctx) if ctx else ('åŒ¿å', 'æœªçŸ¥')
    
    # è·å–project_id
    project_id = get_project_id_from_url(url)
    if not project_id:
        return {"status": "error", "message": "æ— æ³•ä»URLè§£æproject_id"}
    
    # è·å–åä½œè€…åˆ—è¡¨
    store = MessageStore(project_id)
    store.record_collaborator(user_name, user_role)
    collaborators = store.get_collaborators()
    
    return {
        "status": "success",
        "project_id": project_id,
        "total": len(collaborators),
        "collaborators": collaborators
    }


if __name__ == "__main__":
    # è¿è¡ŒMCPæœåŠ¡å™¨
    # ä½¿ç”¨HTTPä¼ è¾“æ–¹å¼ï¼Œç›‘å¬8000ç«¯å£
    mcp.run(transport="http", path="/mcp", host="0.0.0.0", port=8000)



